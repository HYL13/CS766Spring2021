{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of preparing the horses and zebra dataset\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from numpy import savez_compressed\n",
    " \n",
    "# load all images in a directory into memory\n",
    "def load_images(path, size=(256,256)):\n",
    "\tdata_list = list()\n",
    "\t# enumerate filenames in directory, assume all are images\n",
    "\tfor filename in listdir(path):\n",
    "\t\t# load and resize the image\n",
    "\t\tpixels = load_img(path + filename, target_size=size)\n",
    "\t\t# convert to numpy array\n",
    "\t\tpixels = img_to_array(pixels)\n",
    "\t\t# store\n",
    "\t\tdata_list.append(pixels)\n",
    "\treturn asarray(data_list)\n",
    " \n",
    "# dataset path\n",
    "path = 'maps/'\n",
    "# load dataset A\n",
    "dataA1 = load_images(path + 'trainA/')\n",
    "dataAB = load_images(path + 'testA/')\n",
    "dataA = vstack((dataA1, dataAB))\n",
    "print('Loaded dataA: ', dataA.shape)\n",
    "# load dataset B\n",
    "dataB1 = load_images(path + 'trainB/')\n",
    "dataB2 = load_images(path + 'testB/')\n",
    "dataB = vstack((dataB1, dataB2))\n",
    "print('Loaded dataB: ', dataB.shape)\n",
    "# save as compressed numpy array\n",
    "filename = 'maps_cycleGAN_256.npz'\n",
    "savez_compressed(filename, dataA, dataB)\n",
    "print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (2194, 256, 256, 3) (2194, 256, 256, 3)\n",
      "109700\n",
      "Step 1 took 87.44078373908997 seconds, dA[0.646,1.043] dB[1.032,0.894] g[21.288,20.048]\n",
      "Step 2 took 34.53094840049744 seconds, dA[0.901,1.098] dB[2.129,1.100] g[20.595,20.737]\n",
      "Step 3 took 40.792741775512695 seconds, dA[2.479,1.025] dB[2.697,1.274] g[21.055,20.368]\n",
      "Step 4 took 42.02374267578125 seconds, dA[2.059,2.355] dB[2.164,1.708] g[20.811,20.753]\n",
      "Step 5 took 36.45275044441223 seconds, dA[0.865,2.810] dB[1.477,0.905] g[18.970,20.097]\n",
      "Step 6 took 35.56735372543335 seconds, dA[1.338,3.049] dB[1.370,0.803] g[19.999,21.543]\n",
      "Step 7 took 36.05255651473999 seconds, dA[0.960,5.231] dB[1.224,1.354] g[20.192,22.406]\n",
      "Step 8 took 36.89954233169556 seconds, dA[0.950,5.256] dB[1.404,3.177] g[20.740,22.943]\n",
      "Step 9 took 36.20101761817932 seconds, dA[0.750,4.549] dB[2.013,1.968] g[21.252,21.614]\n",
      "Step 10 took 36.89896082878113 seconds, dA[0.661,3.563] dB[1.106,1.601] g[18.930,19.209]\n",
      "Step 11 took 34.02974987030029 seconds, dA[0.611,1.838] dB[0.765,0.790] g[18.407,18.043]\n",
      "Step 12 took 32.79043436050415 seconds, dA[0.908,0.929] dB[1.147,1.118] g[18.649,19.684]\n",
      "Step 13 took 32.201507329940796 seconds, dA[0.698,1.068] dB[0.917,1.507] g[17.668,16.400]\n",
      "Step 14 took 32.237359285354614 seconds, dA[0.635,0.861] dB[0.950,4.858] g[23.972,16.953]\n",
      "Step 15 took 32.64309096336365 seconds, dA[0.473,0.793] dB[1.366,6.111] g[30.321,16.572]\n",
      "Step 16 took 33.42763876914978 seconds, dA[0.687,0.777] dB[3.798,4.126] g[25.651,15.542]\n",
      "Step 17 took 33.2912483215332 seconds, dA[0.522,0.983] dB[6.625,1.795] g[20.314,16.716]\n",
      "Step 18 took 32.89815402030945 seconds, dA[0.704,1.027] dB[1.744,0.770] g[17.667,15.844]\n",
      "Step 19 took 32.87070345878601 seconds, dA[0.500,0.751] dB[0.765,0.889] g[18.120,15.564]\n",
      "Step 20 took 32.403106451034546 seconds, dA[0.524,0.691] dB[0.327,0.455] g[17.569,16.269]\n",
      "Step 21 took 32.531434059143066 seconds, dA[0.594,0.936] dB[0.671,0.456] g[16.626,15.965]\n",
      "Step 22 took 32.11129951477051 seconds, dA[0.582,0.884] dB[0.440,0.489] g[16.717,15.660]\n",
      "Step 23 took 33.0689218044281 seconds, dA[0.483,0.674] dB[0.929,0.413] g[15.770,14.693]\n",
      "Step 24 took 32.501710414886475 seconds, dA[0.511,0.837] dB[0.521,0.457] g[15.233,13.774]\n",
      "Step 25 took 33.207545042037964 seconds, dA[0.655,2.008] dB[0.716,0.558] g[13.501,13.995]\n",
      "Step 26 took 33.01354384422302 seconds, dA[0.921,0.707] dB[0.396,0.391] g[17.335,17.249]\n",
      "Step 27 took 32.46888852119446 seconds, dA[1.038,0.559] dB[0.327,0.522] g[17.116,16.369]\n",
      "Step 28 took 34.13152289390564 seconds, dA[0.528,0.772] dB[0.582,0.309] g[16.096,15.459]\n",
      "Step 29 took 32.61264634132385 seconds, dA[0.511,0.942] dB[0.405,0.360] g[16.809,16.046]\n",
      "Step 30 took 33.44770836830139 seconds, dA[0.406,0.713] dB[0.229,0.486] g[17.138,16.531]\n",
      "Step 31 took 31.71364426612854 seconds, dA[0.602,1.495] dB[0.756,0.478] g[15.570,16.012]\n",
      "Step 32 took 31.635531425476074 seconds, dA[0.947,1.272] dB[0.724,0.519] g[14.131,14.528]\n",
      "Step 33 took 31.777849912643433 seconds, dA[0.757,1.324] dB[0.261,0.558] g[15.483,15.053]\n",
      "Step 34 took 31.04283595085144 seconds, dA[0.611,1.516] dB[0.255,0.364] g[15.581,15.288]\n",
      "Step 35 took 31.05086588859558 seconds, dA[0.640,1.020] dB[0.466,0.388] g[15.063,14.261]\n",
      "Step 36 took 30.476081132888794 seconds, dA[0.616,2.773] dB[0.597,0.325] g[14.195,15.935]\n",
      "Step 37 took 30.994108200073242 seconds, dA[0.795,1.203] dB[0.243,0.463] g[14.954,14.151]\n",
      "Step 38 took 31.7044460773468 seconds, dA[1.154,0.833] dB[0.200,0.473] g[14.919,13.694]\n",
      "Step 39 took 31.176703691482544 seconds, dA[0.877,1.971] dB[0.228,0.374] g[15.042,15.127]\n",
      "Step 40 took 31.220112323760986 seconds, dA[0.630,1.220] dB[0.220,0.648] g[14.068,12.393]\n",
      "Step 41 took 31.67771339416504 seconds, dA[0.583,1.250] dB[0.216,0.402] g[15.161,14.257]\n",
      "Step 42 took 30.57599711418152 seconds, dA[0.388,0.923] dB[0.111,0.399] g[15.558,14.212]\n",
      "Step 43 took 31.1200008392334 seconds, dA[0.444,1.776] dB[0.240,0.244] g[15.630,15.274]\n",
      "Step 44 took 31.32002830505371 seconds, dA[0.492,0.731] dB[0.350,0.215] g[12.879,12.836]\n",
      "Step 45 took 31.263780117034912 seconds, dA[0.511,0.894] dB[0.250,0.297] g[13.471,13.257]\n",
      "Step 46 took 31.284798860549927 seconds, dA[0.418,0.462] dB[0.366,0.274] g[12.896,12.654]\n",
      "Step 47 took 30.951033353805542 seconds, dA[0.454,0.824] dB[0.463,0.175] g[14.027,12.630]\n",
      "Step 48 took 31.030486583709717 seconds, dA[0.418,0.683] dB[0.421,0.281] g[12.751,12.273]\n",
      "Step 49 took 31.023682594299316 seconds, dA[0.387,0.402] dB[0.238,0.285] g[14.724,13.254]\n",
      "Step 50 took 31.56631827354431 seconds, dA[0.339,0.406] dB[0.363,0.281] g[14.410,13.248]\n",
      "Step 51 took 31.890772819519043 seconds, dA[0.372,0.633] dB[0.176,0.215] g[15.323,14.120]\n",
      "Step 52 took 31.014564037322998 seconds, dA[0.263,0.821] dB[0.326,0.156] g[14.401,14.901]\n",
      "Step 53 took 31.184966325759888 seconds, dA[0.454,0.366] dB[0.235,0.264] g[14.623,13.505]\n",
      "Step 54 took 30.85060977935791 seconds, dA[0.506,0.541] dB[0.212,0.255] g[14.611,13.674]\n",
      "Step 55 took 31.370286226272583 seconds, dA[0.399,0.625] dB[0.156,0.164] g[14.830,13.356]\n",
      "Step 56 took 31.39259147644043 seconds, dA[0.399,0.850] dB[0.343,0.124] g[12.127,11.974]\n",
      "Step 57 took 31.01166868209839 seconds, dA[0.346,0.771] dB[0.314,0.218] g[14.111,13.166]\n",
      "Step 58 took 31.08765411376953 seconds, dA[0.427,0.656] dB[0.135,0.199] g[15.525,15.304]\n",
      "Step 59 took 31.170618772506714 seconds, dA[0.252,0.574] dB[0.289,0.209] g[15.174,15.466]\n",
      "Step 60 took 30.97890067100525 seconds, dA[0.281,0.321] dB[0.224,0.134] g[13.337,13.326]\n",
      "Step 61 took 31.092723608016968 seconds, dA[0.430,0.730] dB[1.124,0.289] g[13.546,12.977]\n",
      "Step 62 took 31.098795413970947 seconds, dA[0.403,0.994] dB[0.584,0.301] g[12.655,12.476]\n",
      "Step 63 took 30.964919328689575 seconds, dA[0.411,0.423] dB[0.293,0.142] g[14.397,12.701]\n",
      "Step 64 took 30.984007835388184 seconds, dA[0.362,0.679] dB[0.243,0.155] g[14.918,13.737]\n",
      "Step 65 took 30.970214366912842 seconds, dA[0.329,0.307] dB[0.360,0.152] g[12.206,11.798]\n",
      "Step 66 took 31.69524335861206 seconds, dA[0.314,0.261] dB[0.184,0.229] g[12.490,11.751]\n",
      "Step 67 took 31.521095275878906 seconds, dA[0.320,0.715] dB[0.154,0.122] g[15.428,14.102]\n",
      "Step 68 took 30.925882577896118 seconds, dA[0.294,0.395] dB[0.215,0.118] g[15.001,13.612]\n",
      "Step 69 took 30.425000190734863 seconds, dA[0.388,0.440] dB[0.135,0.284] g[14.611,12.917]\n",
      "Step 70 took 30.806914567947388 seconds, dA[0.342,0.638] dB[0.145,0.216] g[11.614,13.054]\n",
      "Step 71 took 30.919933319091797 seconds, dA[0.398,0.414] dB[0.126,0.166] g[14.345,13.005]\n",
      "Step 72 took 30.698770999908447 seconds, dA[0.364,0.608] dB[0.087,0.141] g[12.431,11.753]\n",
      "Step 73 took 30.440096616744995 seconds, dA[0.363,0.350] dB[0.077,0.130] g[14.563,12.994]\n",
      "Step 74 took 31.561851024627686 seconds, dA[0.327,0.538] dB[0.161,0.139] g[13.723,12.026]\n",
      "Step 75 took 31.359493017196655 seconds, dA[0.321,0.685] dB[0.115,0.214] g[14.197,12.585]\n",
      "Step 76 took 30.62513780593872 seconds, dA[0.515,0.402] dB[0.092,0.262] g[13.308,11.344]\n",
      "Step 77 took 30.750300884246826 seconds, dA[0.289,0.333] dB[0.108,0.267] g[17.218,16.916]\n",
      "Step 78 took 31.35800838470459 seconds, dA[0.573,0.360] dB[0.105,0.247] g[14.560,13.120]\n",
      "Step 79 took 31.147979974746704 seconds, dA[0.411,0.844] dB[0.136,0.240] g[15.186,14.574]\n",
      "Step 80 took 30.824865341186523 seconds, dA[0.327,0.438] dB[0.162,0.250] g[13.587,12.321]\n",
      "Step 81 took 30.95971655845642 seconds, dA[0.336,0.750] dB[0.086,0.149] g[14.553,14.263]\n",
      "Step 82 took 31.072877168655396 seconds, dA[0.366,0.507] dB[0.164,0.142] g[13.645,12.701]\n",
      "Step 83 took 31.242233753204346 seconds, dA[0.619,0.285] dB[0.086,0.129] g[11.660,11.289]\n",
      "Step 84 took 30.80187749862671 seconds, dA[0.311,0.432] dB[0.108,0.121] g[11.747,10.847]\n",
      "Step 85 took 30.652268171310425 seconds, dA[0.218,0.555] dB[0.133,0.062] g[14.227,12.439]\n",
      "Step 86 took 30.97263526916504 seconds, dA[0.381,0.285] dB[0.059,0.116] g[14.018,12.382]\n",
      "Step 87 took 30.694151163101196 seconds, dA[0.251,0.197] dB[0.075,0.067] g[14.794,13.210]\n",
      "Step 88 took 30.998746395111084 seconds, dA[0.231,0.308] dB[0.176,0.128] g[10.956,10.134]\n",
      "Step 89 took 30.960289239883423 seconds, dA[0.205,0.196] dB[0.144,0.096] g[13.213,12.194]\n",
      "Step 90 took 31.163013219833374 seconds, dA[0.210,0.176] dB[0.093,0.062] g[15.281,14.415]\n",
      "Step 91 took 31.707411527633667 seconds, dA[0.245,0.282] dB[0.071,0.082] g[14.193,12.409]\n",
      "Step 92 took 30.867568254470825 seconds, dA[0.298,0.149] dB[0.087,0.203] g[14.543,12.990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 93 took 31.847293853759766 seconds, dA[0.191,0.244] dB[0.062,0.194] g[14.287,12.371]\n",
      "Step 94 took 30.430177450180054 seconds, dA[0.251,0.287] dB[0.079,0.161] g[14.470,12.747]\n",
      "Step 95 took 30.610000133514404 seconds, dA[0.248,0.298] dB[0.099,0.122] g[14.526,12.916]\n",
      "Step 96 took 30.509997606277466 seconds, dA[0.178,0.153] dB[0.215,0.125] g[13.118,12.467]\n",
      "Step 97 took 30.416001558303833 seconds, dA[0.220,0.285] dB[0.111,0.197] g[11.630,11.065]\n",
      "Step 98 took 30.343268632888794 seconds, dA[0.221,0.277] dB[0.248,0.056] g[13.652,11.712]\n",
      "Step 99 took 30.22508215904236 seconds, dA[0.228,0.190] dB[0.098,0.118] g[13.226,11.318]\n",
      "Step 100 took 30.386000871658325 seconds, dA[0.140,0.417] dB[0.082,0.093] g[15.065,14.292]\n",
      ">Saved: g_model_AtoB_000100.h5 and g_model_BtoA_000100.h5\n",
      "Step 101 took 30.18367338180542 seconds, dA[0.299,0.165] dB[0.066,0.071] g[13.628,12.064]\n",
      "Step 102 took 30.999128818511963 seconds, dA[0.149,0.105] dB[0.109,0.097] g[11.057,9.674]\n",
      "Step 103 took 30.560001373291016 seconds, dA[0.110,0.164] dB[0.094,0.135] g[12.033,10.098]\n",
      "Step 104 took 30.548998594284058 seconds, dA[0.160,0.220] dB[0.072,0.075] g[12.722,10.330]\n",
      "Step 105 took 30.035999298095703 seconds, dA[0.140,0.186] dB[0.073,0.105] g[14.505,12.837]\n",
      "Step 106 took 30.8370578289032 seconds, dA[0.344,0.053] dB[0.084,0.058] g[12.788,11.631]\n",
      "Step 107 took 31.078603982925415 seconds, dA[0.161,0.272] dB[0.107,0.133] g[14.508,13.599]\n",
      "Step 108 took 30.71814203262329 seconds, dA[0.172,0.183] dB[0.119,0.046] g[13.690,12.226]\n",
      "Step 109 took 31.898348808288574 seconds, dA[0.215,0.114] dB[0.092,0.116] g[14.745,13.700]\n",
      "Step 110 took 30.900375843048096 seconds, dA[0.255,0.208] dB[0.064,0.254] g[13.901,12.471]\n",
      "Step 111 took 32.13902497291565 seconds, dA[0.142,0.215] dB[0.089,0.088] g[15.530,15.024]\n",
      "Step 112 took 31.696114778518677 seconds, dA[0.178,0.100] dB[0.101,0.067] g[13.694,12.346]\n",
      "Step 113 took 30.983136653900146 seconds, dA[0.206,0.201] dB[0.132,0.115] g[14.318,13.290]\n",
      "Step 114 took 31.232658863067627 seconds, dA[0.089,0.101] dB[0.139,0.077] g[13.392,12.614]\n",
      "Step 115 took 31.592188835144043 seconds, dA[0.187,0.388] dB[0.085,0.087] g[13.415,11.851]\n",
      "Step 116 took 31.191635131835938 seconds, dA[0.153,0.131] dB[0.159,0.061] g[12.845,12.116]\n",
      "Step 117 took 30.897567987442017 seconds, dA[0.240,0.147] dB[0.086,0.101] g[11.910,11.002]\n",
      "Step 118 took 30.229106664657593 seconds, dA[0.165,0.343] dB[0.046,0.129] g[13.671,11.750]\n",
      "Step 119 took 30.18899917602539 seconds, dA[0.196,0.158] dB[0.050,0.063] g[13.518,12.267]\n",
      "Step 120 took 30.27700161933899 seconds, dA[0.274,0.120] dB[0.068,0.068] g[11.829,11.875]\n",
      "Step 121 took 30.244835138320923 seconds, dA[0.201,0.075] dB[0.080,0.031] g[13.724,12.127]\n",
      "Step 122 took 30.327001333236694 seconds, dA[0.209,0.042] dB[0.092,0.130] g[12.290,12.025]\n",
      "Step 123 took 30.234999418258667 seconds, dA[0.229,0.192] dB[0.056,0.144] g[13.677,12.853]\n",
      "Step 124 took 30.27599811553955 seconds, dA[0.258,0.181] dB[0.077,0.161] g[13.826,12.495]\n",
      "Step 125 took 31.226881980895996 seconds, dA[0.160,0.204] dB[0.107,0.167] g[15.121,14.022]\n",
      "Step 126 took 31.76833176612854 seconds, dA[0.149,0.116] dB[0.173,0.104] g[14.596,13.003]\n",
      "Step 127 took 31.88035750389099 seconds, dA[0.250,0.097] dB[0.194,0.096] g[13.088,11.991]\n",
      "Step 128 took 30.724689245224 seconds, dA[0.147,0.320] dB[0.040,0.082] g[14.643,13.565]\n",
      "Step 129 took 30.268468141555786 seconds, dA[0.158,0.056] dB[0.088,0.079] g[13.302,13.413]\n",
      "Step 130 took 30.51499891281128 seconds, dA[0.350,0.039] dB[0.143,0.059] g[12.573,11.576]\n",
      "Step 131 took 31.238521099090576 seconds, dA[0.243,0.236] dB[0.067,0.073] g[13.055,11.274]\n",
      "Step 132 took 30.77453923225403 seconds, dA[0.129,0.139] dB[0.096,0.058] g[12.561,11.473]\n",
      "Step 133 took 30.861818075180054 seconds, dA[0.087,0.072] dB[0.100,0.072] g[14.041,13.042]\n",
      "Step 134 took 30.203407049179077 seconds, dA[0.321,0.104] dB[0.062,0.076] g[13.615,12.302]\n",
      "Step 135 took 30.335002899169922 seconds, dA[0.158,0.048] dB[0.051,0.054] g[13.384,11.505]\n",
      "Step 136 took 31.022602081298828 seconds, dA[0.072,0.090] dB[0.062,0.033] g[13.980,12.172]\n",
      "Step 137 took 31.21329617500305 seconds, dA[0.122,0.080] dB[0.041,0.070] g[14.935,13.811]\n",
      "Step 138 took 31.447700023651123 seconds, dA[0.403,0.080] dB[0.072,0.042] g[11.898,10.536]\n",
      "Step 139 took 30.926884651184082 seconds, dA[0.132,0.087] dB[0.093,0.058] g[10.476,9.819]\n",
      "Step 140 took 30.97265911102295 seconds, dA[0.107,0.042] dB[0.096,0.106] g[14.110,12.346]\n",
      "Step 141 took 31.02440333366394 seconds, dA[0.073,0.201] dB[0.094,0.062] g[14.910,13.621]\n",
      "Step 142 took 31.104764699935913 seconds, dA[0.217,0.045] dB[0.130,0.062] g[13.331,11.511]\n",
      "Step 143 took 30.805968761444092 seconds, dA[0.309,0.099] dB[0.106,0.038] g[12.750,10.824]\n",
      "Step 144 took 31.424172163009644 seconds, dA[0.126,0.122] dB[0.086,0.168] g[14.023,12.761]\n",
      "Step 145 took 30.923229217529297 seconds, dA[0.148,0.158] dB[0.067,0.075] g[14.248,12.234]\n",
      "Step 146 took 30.927666902542114 seconds, dA[0.126,0.035] dB[0.085,0.064] g[14.050,12.131]\n",
      "Step 147 took 30.90724802017212 seconds, dA[0.110,0.120] dB[0.083,0.083] g[13.547,12.254]\n",
      "Step 148 took 30.88341498374939 seconds, dA[0.294,0.115] dB[0.079,0.072] g[12.966,11.319]\n",
      "Step 149 took 31.65364384651184 seconds, dA[0.179,0.204] dB[0.074,0.115] g[13.872,11.864]\n",
      "Step 150 took 31.208377599716187 seconds, dA[0.173,0.103] dB[0.057,0.045] g[12.729,11.421]\n",
      "Step 151 took 32.36153793334961 seconds, dA[0.153,0.310] dB[0.072,0.079] g[12.054,11.558]\n",
      "Step 152 took 32.1607563495636 seconds, dA[0.180,0.089] dB[0.063,0.103] g[13.183,11.817]\n",
      "Step 153 took 33.26244521141052 seconds, dA[0.128,0.085] dB[0.056,0.062] g[13.823,12.012]\n",
      "Step 154 took 31.50454330444336 seconds, dA[0.378,0.036] dB[0.061,0.042] g[13.983,13.935]\n",
      "Step 155 took 30.579893350601196 seconds, dA[0.223,0.136] dB[0.062,0.070] g[13.850,12.941]\n",
      "Step 156 took 31.30284547805786 seconds, dA[0.296,0.039] dB[0.066,0.058] g[13.596,12.429]\n",
      "Step 157 took 31.06128478050232 seconds, dA[0.170,0.470] dB[0.094,0.084] g[13.691,12.435]\n",
      "Step 158 took 30.863131523132324 seconds, dA[0.307,0.059] dB[0.140,0.097] g[11.787,10.291]\n",
      "Step 159 took 30.73177719116211 seconds, dA[0.095,0.040] dB[0.083,0.033] g[11.197,10.572]\n",
      "Step 160 took 31.080023765563965 seconds, dA[0.134,0.138] dB[0.052,0.245] g[13.274,11.954]\n",
      "Step 161 took 32.787033796310425 seconds, dA[0.121,0.030] dB[0.101,0.072] g[14.655,13.384]\n",
      "Step 162 took 32.183589935302734 seconds, dA[0.119,0.105] dB[0.248,0.148] g[14.479,14.066]\n",
      "Step 163 took 31.717042446136475 seconds, dA[0.135,0.317] dB[0.048,0.146] g[13.825,12.192]\n",
      "Step 164 took 32.24172282218933 seconds, dA[0.194,0.040] dB[0.145,0.213] g[13.157,12.107]\n",
      "Step 165 took 31.15962553024292 seconds, dA[0.137,0.123] dB[0.129,0.129] g[13.052,11.566]\n",
      "Step 166 took 31.70278763771057 seconds, dA[0.180,0.055] dB[0.063,0.074] g[13.016,10.941]\n",
      "Step 167 took 31.581817388534546 seconds, dA[0.092,0.049] dB[0.046,0.041] g[11.444,11.035]\n",
      "Step 168 took 31.404874563217163 seconds, dA[0.046,0.056] dB[0.070,0.104] g[14.344,13.525]\n",
      "Step 169 took 31.375099658966064 seconds, dA[0.203,0.109] dB[0.082,0.069] g[12.928,10.805]\n",
      "Step 170 took 31.19010281562805 seconds, dA[0.102,0.130] dB[0.118,0.146] g[13.930,12.940]\n",
      "Step 171 took 30.303332567214966 seconds, dA[0.219,0.036] dB[0.034,0.049] g[12.953,11.641]\n",
      "Step 172 took 30.49600100517273 seconds, dA[0.090,0.068] dB[0.052,0.174] g[13.389,12.597]\n",
      "Step 173 took 30.371997833251953 seconds, dA[0.079,0.120] dB[0.030,0.074] g[14.410,12.891]\n",
      "Step 174 took 30.41600227355957 seconds, dA[0.343,0.035] dB[0.039,0.108] g[13.299,10.947]\n",
      "Step 175 took 30.612804174423218 seconds, dA[0.118,0.390] dB[0.040,0.075] g[13.264,11.304]\n",
      "Step 176 took 30.279999256134033 seconds, dA[0.084,0.148] dB[0.047,0.086] g[13.775,12.193]\n",
      "Step 177 took 30.26570200920105 seconds, dA[0.203,0.023] dB[0.128,0.107] g[12.846,12.121]\n",
      "Step 178 took 31.026049375534058 seconds, dA[0.089,0.038] dB[0.045,0.082] g[14.193,13.150]\n",
      "Step 179 took 31.12225842475891 seconds, dA[0.349,0.118] dB[0.079,0.064] g[11.705,10.604]\n",
      "Step 180 took 31.96227502822876 seconds, dA[0.062,0.126] dB[0.038,0.096] g[14.400,13.888]\n",
      "Step 181 took 32.31799817085266 seconds, dA[0.176,0.116] dB[0.034,0.097] g[13.200,11.232]\n",
      "Step 182 took 33.07300019264221 seconds, dA[0.114,0.072] dB[0.043,0.150] g[13.068,11.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 183 took 32.33634543418884 seconds, dA[0.039,0.026] dB[0.044,0.107] g[12.061,11.953]\n",
      "Step 184 took 32.33200001716614 seconds, dA[0.105,0.038] dB[0.039,0.049] g[11.263,11.556]\n",
      "Step 185 took 31.116063833236694 seconds, dA[0.093,0.077] dB[0.060,0.276] g[9.747,9.815]\n",
      "Step 186 took 31.09609055519104 seconds, dA[0.200,0.428] dB[0.063,0.125] g[15.482,14.705]\n",
      "Step 187 took 32.00237488746643 seconds, dA[0.195,0.064] dB[0.045,0.056] g[15.971,14.999]\n",
      "Step 188 took 34.570515871047974 seconds, dA[0.083,0.071] dB[0.033,0.132] g[15.159,14.597]\n",
      "Step 189 took 35.29652500152588 seconds, dA[0.120,0.159] dB[0.054,0.126] g[13.944,11.690]\n",
      "Step 190 took 34.80217003822327 seconds, dA[0.134,0.110] dB[0.032,0.079] g[14.062,12.066]\n",
      "Step 191 took 33.518611669540405 seconds, dA[0.085,0.027] dB[0.033,0.075] g[14.257,13.254]\n",
      "Step 192 took 31.72300434112549 seconds, dA[0.190,0.074] dB[0.032,0.060] g[13.425,11.560]\n",
      "Step 193 took 31.29858374595642 seconds, dA[0.051,0.240] dB[0.040,0.387] g[11.198,11.196]\n",
      "Step 194 took 31.803595304489136 seconds, dA[0.110,0.054] dB[0.066,0.047] g[11.916,11.360]\n",
      "Step 195 took 31.01705002784729 seconds, dA[0.200,0.233] dB[0.037,0.038] g[12.842,11.447]\n",
      "Step 196 took 30.96899914741516 seconds, dA[0.147,0.204] dB[0.025,0.072] g[11.843,10.680]\n",
      "Step 197 took 31.298043489456177 seconds, dA[0.070,0.045] dB[0.039,0.264] g[10.104,9.892]\n",
      "Step 198 took 31.66777467727661 seconds, dA[0.060,0.042] dB[0.098,0.104] g[13.764,12.560]\n",
      "Step 199 took 30.946715116500854 seconds, dA[0.128,0.070] dB[0.126,0.113] g[12.021,10.968]\n",
      "Step 200 took 30.64788556098938 seconds, dA[0.066,0.055] dB[0.023,0.135] g[12.115,11.360]\n",
      ">Saved: g_model_AtoB_000200.h5 and g_model_BtoA_000200.h5\n",
      "Step 201 took 30.98347020149231 seconds, dA[0.132,0.044] dB[0.065,0.086] g[13.646,11.581]\n",
      "Step 202 took 30.64298129081726 seconds, dA[0.080,0.117] dB[0.039,0.081] g[10.933,9.941]\n",
      "Step 203 took 30.523001670837402 seconds, dA[0.051,0.113] dB[0.063,0.047] g[14.175,13.767]\n",
      "Step 204 took 30.62837266921997 seconds, dA[0.128,0.119] dB[0.047,0.193] g[13.848,12.738]\n",
      "Step 205 took 30.56624674797058 seconds, dA[0.240,0.588] dB[0.057,0.067] g[13.299,11.820]\n",
      "Step 206 took 31.01458168029785 seconds, dA[0.158,0.174] dB[0.037,0.127] g[13.326,13.085]\n",
      "Step 207 took 30.71131157875061 seconds, dA[0.534,0.204] dB[0.065,0.138] g[10.519,10.078]\n",
      "Step 208 took 31.417297840118408 seconds, dA[0.162,0.067] dB[0.070,0.172] g[14.367,13.491]\n",
      "Step 209 took 31.242313623428345 seconds, dA[0.455,0.022] dB[0.039,0.105] g[12.735,11.409]\n",
      "Step 210 took 31.139549016952515 seconds, dA[0.064,0.089] dB[0.043,0.048] g[13.535,11.866]\n",
      "Step 211 took 31.02899980545044 seconds, dA[0.038,0.062] dB[0.051,0.208] g[12.245,11.866]\n",
      "Step 212 took 31.064000368118286 seconds, dA[0.103,0.048] dB[0.177,0.180] g[11.500,11.003]\n",
      "Step 213 took 30.694280862808228 seconds, dA[0.121,0.141] dB[0.093,0.156] g[13.875,12.562]\n",
      "Step 214 took 30.69632577896118 seconds, dA[0.130,0.021] dB[0.048,0.146] g[13.945,12.186]\n",
      "Step 215 took 30.902776956558228 seconds, dA[0.176,0.171] dB[0.068,0.218] g[10.172,9.036]\n",
      "Step 216 took 30.63599944114685 seconds, dA[0.090,0.048] dB[0.092,0.174] g[13.125,11.683]\n",
      "Step 217 took 30.608813047409058 seconds, dA[0.117,0.299] dB[0.058,0.103] g[12.915,11.164]\n",
      "Step 218 took 30.658000230789185 seconds, dA[0.078,0.092] dB[0.157,0.125] g[12.454,11.932]\n",
      "Step 219 took 30.73299813270569 seconds, dA[0.168,0.016] dB[0.059,0.202] g[12.280,11.475]\n",
      "Step 220 took 30.58307409286499 seconds, dA[0.077,0.028] dB[0.055,0.139] g[10.654,10.076]\n",
      "Step 221 took 30.649669408798218 seconds, dA[0.156,0.180] dB[0.057,0.223] g[8.636,8.351]\n",
      "Step 222 took 30.62999963760376 seconds, dA[0.075,0.048] dB[0.050,0.087] g[13.366,12.196]\n",
      "Step 223 took 30.612000703811646 seconds, dA[0.163,0.026] dB[0.050,0.062] g[12.348,10.860]\n",
      "Step 224 took 30.759749174118042 seconds, dA[0.073,0.186] dB[0.038,0.167] g[12.256,10.548]\n",
      "Step 225 took 30.706037759780884 seconds, dA[0.093,0.066] dB[0.032,0.091] g[11.169,10.714]\n",
      "Step 226 took 30.63699984550476 seconds, dA[0.230,0.174] dB[0.052,0.112] g[13.612,12.028]\n",
      "Step 227 took 30.692002534866333 seconds, dA[0.044,0.045] dB[0.045,0.162] g[14.170,14.039]\n",
      "Step 228 took 30.65566325187683 seconds, dA[0.212,0.043] dB[0.089,0.114] g[9.701,9.233]\n",
      "Step 229 took 30.683000564575195 seconds, dA[0.104,0.247] dB[0.066,0.074] g[12.786,11.121]\n",
      "Step 230 took 30.731000423431396 seconds, dA[0.109,0.045] dB[0.053,0.066] g[14.584,13.940]\n",
      "Step 231 took 30.610999822616577 seconds, dA[0.137,0.040] dB[0.045,0.076] g[12.860,11.844]\n",
      "Step 232 took 30.94233751296997 seconds, dA[0.105,0.033] dB[0.062,0.077] g[11.928,10.571]\n",
      "Step 233 took 30.72699809074402 seconds, dA[0.044,0.058] dB[0.057,0.079] g[11.537,11.371]\n",
      "Step 234 took 30.768046855926514 seconds, dA[0.123,0.043] dB[0.037,0.039] g[13.099,11.491]\n",
      "Step 235 took 30.62978768348694 seconds, dA[0.094,0.148] dB[0.039,0.135] g[12.412,11.230]\n",
      "Step 236 took 32.35133361816406 seconds, dA[0.079,0.022] dB[0.041,0.081] g[13.554,12.007]\n",
      "Step 237 took 31.889281511306763 seconds, dA[0.052,0.090] dB[0.040,0.058] g[14.412,13.741]\n",
      "Step 238 took 31.1251802444458 seconds, dA[0.055,0.086] dB[0.098,0.094] g[11.801,12.089]\n",
      "Step 239 took 31.20100212097168 seconds, dA[0.083,0.041] dB[0.034,0.067] g[15.061,14.598]\n",
      "Step 240 took 31.219443559646606 seconds, dA[0.178,0.083] dB[0.034,0.069] g[12.841,12.079]\n",
      "Step 241 took 30.283286809921265 seconds, dA[0.111,0.193] dB[0.032,0.049] g[11.307,10.375]\n",
      "Step 242 took 30.31999969482422 seconds, dA[0.131,0.083] dB[0.028,0.101] g[13.236,12.226]\n",
      "Step 243 took 30.3480007648468 seconds, dA[0.080,0.066] dB[0.047,0.065] g[13.208,11.920]\n",
      "Step 244 took 30.13289999961853 seconds, dA[0.098,0.029] dB[0.022,0.103] g[11.146,10.065]\n",
      "Step 245 took 30.542611837387085 seconds, dA[0.212,0.080] dB[0.034,0.113] g[13.772,12.367]\n",
      "Step 246 took 30.526999473571777 seconds, dA[0.251,0.044] dB[0.055,0.071] g[10.698,10.599]\n",
      "Step 247 took 31.137001037597656 seconds, dA[0.113,0.032] dB[0.043,0.179] g[10.175,10.458]\n",
      "Step 248 took 31.09576153755188 seconds, dA[0.465,0.036] dB[0.017,0.120] g[10.567,9.480]\n",
      "Step 249 took 30.74499797821045 seconds, dA[0.092,0.034] dB[0.131,0.066] g[12.465,10.561]\n",
      "Step 250 took 30.556999444961548 seconds, dA[0.161,0.148] dB[0.042,0.116] g[13.222,12.300]\n",
      "Step 251 took 30.828998565673828 seconds, dA[0.272,0.066] dB[0.125,0.044] g[12.494,11.659]\n",
      "Step 252 took 30.563987016677856 seconds, dA[0.102,0.246] dB[0.053,0.031] g[13.147,11.753]\n",
      "Step 253 took 31.110997438430786 seconds, dA[0.128,0.189] dB[0.048,0.063] g[12.814,10.962]\n",
      "Step 254 took 30.936999797821045 seconds, dA[0.093,0.046] dB[0.053,0.054] g[11.929,10.988]\n",
      "Step 255 took 30.826035976409912 seconds, dA[0.150,0.122] dB[0.024,0.113] g[10.933,10.284]\n",
      "Step 256 took 30.245580434799194 seconds, dA[0.214,0.204] dB[0.026,0.027] g[12.148,10.195]\n",
      "Step 257 took 30.578999280929565 seconds, dA[0.138,0.056] dB[0.054,0.013] g[10.464,9.394]\n",
      "Step 258 took 30.415000438690186 seconds, dA[0.048,0.063] dB[0.029,0.032] g[12.479,11.649]\n",
      "Step 259 took 31.01102876663208 seconds, dA[0.389,0.052] dB[0.033,0.076] g[11.706,10.206]\n",
      "Step 260 took 30.31387972831726 seconds, dA[0.115,0.167] dB[0.052,0.045] g[14.014,14.414]\n",
      "Step 261 took 30.451032876968384 seconds, dA[0.093,0.030] dB[0.055,0.050] g[11.133,9.488]\n",
      "Step 262 took 30.290998458862305 seconds, dA[0.096,0.242] dB[0.029,0.106] g[12.374,10.999]\n",
      "Step 263 took 30.3820378780365 seconds, dA[0.142,0.347] dB[0.026,0.040] g[12.690,10.629]\n",
      "Step 264 took 30.225048780441284 seconds, dA[0.225,0.026] dB[0.025,0.052] g[11.928,10.258]\n",
      "Step 265 took 30.294999599456787 seconds, dA[0.080,0.062] dB[0.022,0.071] g[13.258,12.255]\n",
      "Step 266 took 30.256001710891724 seconds, dA[0.040,0.042] dB[0.036,0.045] g[12.483,11.768]\n",
      "Step 267 took 30.264132976531982 seconds, dA[0.064,0.191] dB[0.041,0.061] g[12.730,10.958]\n",
      "Step 268 took 30.232157468795776 seconds, dA[0.112,0.315] dB[0.034,0.109] g[14.239,14.275]\n",
      "Step 269 took 30.343998670578003 seconds, dA[0.208,0.232] dB[0.046,0.046] g[13.199,11.513]\n",
      "Step 270 took 30.846999406814575 seconds, dA[0.114,0.021] dB[0.043,0.034] g[9.199,9.589]\n",
      "Step 271 took 30.77551817893982 seconds, dA[0.134,0.046] dB[0.034,0.042] g[10.728,10.255]\n",
      "Step 272 took 30.603036642074585 seconds, dA[0.131,0.102] dB[0.035,0.059] g[12.600,11.339]\n",
      "Step 273 took 30.94100022315979 seconds, dA[0.027,0.033] dB[0.029,0.104] g[12.534,11.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 274 took 30.726999759674072 seconds, dA[0.040,0.020] dB[0.036,0.064] g[13.934,12.853]\n",
      "Step 275 took 30.319092988967896 seconds, dA[0.054,0.013] dB[0.053,0.037] g[13.795,12.615]\n",
      "Step 276 took 30.567384958267212 seconds, dA[0.162,0.105] dB[0.120,0.283] g[9.982,9.317]\n",
      "Step 277 took 30.694000482559204 seconds, dA[0.092,0.031] dB[0.039,0.022] g[12.434,10.874]\n",
      "Step 278 took 30.812000274658203 seconds, dA[0.055,0.088] dB[0.037,0.084] g[12.614,10.864]\n",
      "Step 279 took 30.997344493865967 seconds, dA[0.110,0.027] dB[0.037,0.103] g[11.294,10.440]\n",
      "Step 280 took 30.280651569366455 seconds, dA[0.038,0.073] dB[0.028,0.054] g[11.047,10.199]\n",
      "Step 281 took 30.109525203704834 seconds, dA[0.195,0.041] dB[0.030,0.048] g[12.213,10.368]\n",
      "Step 282 took 30.378999948501587 seconds, dA[0.063,0.037] dB[0.065,0.111] g[10.116,9.627]\n",
      "Step 283 took 30.749247550964355 seconds, dA[0.049,0.058] dB[0.026,0.045] g[11.151,10.051]\n",
      "Step 284 took 30.778000831604004 seconds, dA[0.042,0.044] dB[0.023,0.051] g[13.708,12.701]\n",
      "Step 285 took 30.555391311645508 seconds, dA[0.052,0.216] dB[0.036,0.070] g[13.212,11.894]\n",
      "Step 286 took 30.774999141693115 seconds, dA[0.195,0.030] dB[0.026,0.041] g[12.014,10.742]\n",
      "Step 287 took 31.36424732208252 seconds, dA[0.065,0.106] dB[0.047,0.033] g[12.720,10.807]\n",
      "Step 288 took 30.872000217437744 seconds, dA[0.092,0.159] dB[0.018,0.032] g[12.963,11.178]\n",
      "Step 289 took 30.87999987602234 seconds, dA[0.201,0.041] dB[0.032,0.043] g[10.288,10.382]\n",
      "Step 290 took 30.213000535964966 seconds, dA[0.175,0.288] dB[0.043,0.016] g[14.099,13.607]\n",
      "Step 291 took 30.23578953742981 seconds, dA[0.140,0.076] dB[0.024,0.056] g[13.475,12.631]\n",
      "Step 292 took 30.25699806213379 seconds, dA[0.064,0.219] dB[0.029,0.094] g[12.675,11.240]\n",
      "Step 293 took 30.651999711990356 seconds, dA[0.070,0.050] dB[0.041,0.025] g[11.789,10.305]\n",
      "Step 294 took 30.28100037574768 seconds, dA[0.156,0.041] dB[0.020,0.041] g[10.959,9.755]\n",
      "Step 295 took 30.848870277404785 seconds, dA[0.067,0.279] dB[0.024,0.038] g[12.784,11.184]\n",
      "Step 296 took 30.52299976348877 seconds, dA[0.065,0.097] dB[0.022,0.099] g[13.271,12.268]\n",
      "Step 297 took 30.195000410079956 seconds, dA[0.412,0.150] dB[0.022,0.042] g[12.027,10.610]\n",
      "Step 298 took 30.315000295639038 seconds, dA[0.059,0.037] dB[0.021,0.018] g[12.694,11.238]\n",
      "Step 299 took 30.238255500793457 seconds, dA[0.031,0.066] dB[0.019,0.149] g[13.126,13.130]\n",
      "Step 300 took 30.264127254486084 seconds, dA[0.012,0.177] dB[0.037,0.041] g[12.449,12.185]\n",
      ">Saved: g_model_AtoB_000300.h5 and g_model_BtoA_000300.h5\n",
      "Step 301 took 30.5249342918396 seconds, dA[0.124,0.118] dB[0.030,0.050] g[13.328,11.529]\n",
      "Step 302 took 30.971234560012817 seconds, dA[0.519,0.087] dB[0.037,0.087] g[11.729,9.954]\n",
      "Step 303 took 30.797842979431152 seconds, dA[0.054,0.187] dB[0.029,0.054] g[13.412,12.177]\n",
      "Step 304 took 30.57099986076355 seconds, dA[0.092,0.051] dB[0.036,0.100] g[13.169,12.594]\n",
      "Step 305 took 30.34299945831299 seconds, dA[0.134,0.078] dB[0.159,0.093] g[10.951,9.970]\n",
      "Step 306 took 30.718605279922485 seconds, dA[0.030,0.085] dB[0.041,0.061] g[9.379,9.004]\n",
      "Step 307 took 30.664334535598755 seconds, dA[0.050,0.048] dB[0.053,0.073] g[8.279,7.997]\n",
      "Step 308 took 30.164000511169434 seconds, dA[0.084,0.051] dB[0.101,0.097] g[11.286,10.157]\n",
      "Step 309 took 30.17300057411194 seconds, dA[0.053,0.083] dB[0.050,0.043] g[10.882,10.091]\n",
      "Step 310 took 30.234683513641357 seconds, dA[0.043,0.041] dB[0.065,0.057] g[11.578,11.031]\n",
      "Step 311 took 30.178075551986694 seconds, dA[0.057,0.068] dB[0.047,0.058] g[13.221,11.443]\n",
      "Step 312 took 30.36100149154663 seconds, dA[0.112,0.063] dB[0.040,0.068] g[12.673,10.993]\n",
      "Step 313 took 30.70599937438965 seconds, dA[0.028,0.038] dB[0.029,0.103] g[10.663,10.465]\n",
      "Step 314 took 30.484264850616455 seconds, dA[0.038,0.062] dB[0.053,0.033] g[12.943,11.424]\n",
      "Step 315 took 30.2270028591156 seconds, dA[0.130,0.020] dB[0.051,0.088] g[12.206,10.675]\n",
      "Step 316 took 30.259000778198242 seconds, dA[0.025,0.051] dB[0.045,0.091] g[12.564,11.219]\n",
      "Step 317 took 30.76800036430359 seconds, dA[0.057,0.197] dB[0.018,0.054] g[14.067,12.912]\n",
      "Step 318 took 30.70670199394226 seconds, dA[0.081,0.060] dB[0.025,0.020] g[13.090,11.048]\n",
      "Step 319 took 30.459306240081787 seconds, dA[0.066,0.069] dB[0.018,0.025] g[13.192,12.203]\n",
      "Step 320 took 30.888038396835327 seconds, dA[0.385,0.115] dB[0.019,0.036] g[12.008,10.284]\n",
      "Step 321 took 31.37951683998108 seconds, dA[0.101,0.027] dB[0.036,0.026] g[10.703,10.129]\n",
      "Step 322 took 30.567489624023438 seconds, dA[0.103,0.015] dB[0.043,0.026] g[12.699,11.916]\n",
      "Step 323 took 31.012638568878174 seconds, dA[0.070,0.112] dB[0.035,0.011] g[12.706,10.947]\n",
      "Step 324 took 30.425002336502075 seconds, dA[0.028,0.039] dB[0.055,0.042] g[11.252,10.557]\n",
      "Step 325 took 33.813857316970825 seconds, dA[0.075,0.041] dB[0.028,0.025] g[14.389,15.292]\n",
      "Step 326 took 31.88330054283142 seconds, dA[0.118,0.056] dB[0.029,0.101] g[11.720,11.019]\n",
      "Step 327 took 31.219677448272705 seconds, dA[0.032,0.045] dB[0.029,0.068] g[13.045,11.761]\n",
      "Step 328 took 30.699312448501587 seconds, dA[0.057,0.072] dB[0.017,0.026] g[13.257,12.200]\n",
      "Step 329 took 30.504000663757324 seconds, dA[0.148,0.097] dB[0.018,0.079] g[11.390,10.303]\n",
      "Step 330 took 30.5123553276062 seconds, dA[0.045,0.021] dB[0.060,0.033] g[9.733,8.814]\n",
      "Step 331 took 30.39900016784668 seconds, dA[0.036,0.035] dB[0.031,0.163] g[11.749,10.824]\n",
      "Step 332 took 30.576000213623047 seconds, dA[0.014,0.170] dB[0.021,0.025] g[14.749,14.506]\n",
      "Step 333 took 30.383002758026123 seconds, dA[0.031,0.049] dB[0.014,0.016] g[13.680,13.157]\n",
      "Step 334 took 30.333874225616455 seconds, dA[0.169,0.033] dB[0.015,0.044] g[12.477,11.124]\n",
      "Step 335 took 31.098681926727295 seconds, dA[0.070,0.345] dB[0.020,0.087] g[12.453,10.385]\n",
      "Step 336 took 30.287998914718628 seconds, dA[0.036,0.053] dB[0.063,0.073] g[11.241,10.677]\n",
      "Step 337 took 30.370999574661255 seconds, dA[0.303,0.096] dB[0.029,0.057] g[12.364,11.524]\n",
      "Step 338 took 30.359210729599 seconds, dA[0.075,0.058] dB[0.027,0.024] g[12.701,11.098]\n",
      "Step 339 took 31.027000427246094 seconds, dA[0.043,0.020] dB[0.021,0.044] g[13.144,12.092]\n",
      "Step 340 took 32.092658281326294 seconds, dA[0.084,0.229] dB[0.041,0.092] g[12.691,10.487]\n",
      "Step 341 took 30.777089595794678 seconds, dA[0.067,0.071] dB[0.024,0.112] g[13.254,12.572]\n",
      "Step 342 took 31.98459482192993 seconds, dA[0.055,0.070] dB[0.058,0.164] g[13.012,11.514]\n",
      "Step 343 took 31.767003059387207 seconds, dA[0.042,0.086] dB[0.071,0.136] g[12.555,10.872]\n",
      "Step 344 took 31.399998903274536 seconds, dA[0.122,0.266] dB[0.025,0.040] g[12.326,10.517]\n",
      "Step 345 took 32.16749358177185 seconds, dA[0.068,0.094] dB[0.046,0.081] g[13.891,13.650]\n",
      "Step 346 took 31.48146891593933 seconds, dA[0.143,0.100] dB[0.044,0.219] g[12.536,10.987]\n",
      "Step 347 took 31.11109733581543 seconds, dA[0.032,0.081] dB[0.067,0.093] g[12.227,10.843]\n",
      "Step 348 took 31.174668073654175 seconds, dA[0.035,0.039] dB[0.030,0.086] g[13.010,11.939]\n",
      "Step 349 took 31.41213846206665 seconds, dA[0.148,0.060] dB[0.070,0.054] g[9.992,8.826]\n",
      "Step 350 took 31.874834775924683 seconds, dA[0.017,0.048] dB[0.028,0.091] g[12.869,11.568]\n",
      "Step 351 took 32.3082332611084 seconds, dA[0.032,0.088] dB[0.020,0.059] g[15.208,14.805]\n",
      "Step 352 took 31.674071788787842 seconds, dA[0.045,0.041] dB[0.016,0.022] g[13.253,12.439]\n",
      "Step 353 took 32.19442534446716 seconds, dA[0.060,0.059] dB[0.029,0.094] g[12.087,10.680]\n",
      "Step 354 took 31.30291175842285 seconds, dA[0.027,0.028] dB[0.009,0.056] g[10.680,9.643]\n",
      "Step 355 took 31.290234327316284 seconds, dA[0.114,0.057] dB[0.023,0.048] g[12.448,10.850]\n",
      "Step 356 took 31.402281999588013 seconds, dA[0.031,0.139] dB[0.020,0.049] g[11.926,11.696]\n",
      "Step 357 took 31.070967197418213 seconds, dA[0.156,0.067] dB[0.019,0.061] g[12.661,11.405]\n",
      "Step 358 took 30.935888290405273 seconds, dA[0.082,0.052] dB[0.014,0.027] g[12.236,10.738]\n",
      "Step 359 took 30.740115880966187 seconds, dA[0.101,0.026] dB[0.062,0.041] g[10.464,9.933]\n",
      "Step 360 took 30.952142477035522 seconds, dA[0.038,0.075] dB[0.053,0.046] g[11.903,10.521]\n",
      "Step 361 took 31.020764589309692 seconds, dA[0.099,0.076] dB[0.080,0.019] g[11.749,10.271]\n",
      "Step 362 took 31.83581304550171 seconds, dA[0.039,0.040] dB[0.022,0.100] g[12.175,11.251]\n",
      "Step 363 took 30.867756605148315 seconds, dA[0.035,0.053] dB[0.022,0.064] g[10.710,9.580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 364 took 30.836352825164795 seconds, dA[0.051,0.097] dB[0.016,0.037] g[12.382,10.482]\n",
      "Step 365 took 30.862327337265015 seconds, dA[0.065,0.028] dB[0.024,0.087] g[12.099,10.915]\n",
      "Step 366 took 31.378682613372803 seconds, dA[0.023,0.028] dB[0.044,0.046] g[13.512,11.849]\n",
      "Step 367 took 31.504424810409546 seconds, dA[0.017,0.047] dB[0.032,0.015] g[11.752,10.561]\n",
      "Step 368 took 30.802034378051758 seconds, dA[0.103,0.038] dB[0.048,0.028] g[9.605,9.788]\n",
      "Step 369 took 30.818028450012207 seconds, dA[0.108,0.050] dB[0.023,0.022] g[11.254,10.281]\n",
      "Step 370 took 30.944559574127197 seconds, dA[0.048,0.042] dB[0.014,0.090] g[11.638,10.215]\n",
      "Step 371 took 30.63314652442932 seconds, dA[0.059,0.129] dB[0.012,0.060] g[10.103,9.196]\n",
      "Step 372 took 30.7622389793396 seconds, dA[0.088,0.087] dB[0.015,0.030] g[10.305,9.207]\n",
      "Step 373 took 30.711000204086304 seconds, dA[0.204,0.124] dB[0.017,0.047] g[11.853,10.708]\n",
      "Step 374 took 31.044649362564087 seconds, dA[0.075,0.102] dB[0.027,0.045] g[12.380,10.632]\n",
      "Step 375 took 31.167311668395996 seconds, dA[0.163,0.019] dB[0.020,0.023] g[12.763,12.136]\n",
      "Step 376 took 31.18650794029236 seconds, dA[0.063,0.101] dB[0.022,0.023] g[11.284,10.969]\n",
      "Step 377 took 31.49544048309326 seconds, dA[0.069,0.060] dB[0.013,0.072] g[9.857,9.859]\n",
      "Step 378 took 31.240934133529663 seconds, dA[0.054,0.100] dB[0.076,0.086] g[9.955,9.175]\n",
      "Step 379 took 32.40519428253174 seconds, dA[0.030,0.047] dB[0.019,0.045] g[12.466,11.138]\n",
      "Step 380 took 32.02033615112305 seconds, dA[0.037,0.024] dB[0.067,0.098] g[9.831,9.935]\n",
      "Step 381 took 31.81511950492859 seconds, dA[0.031,0.022] dB[0.057,0.088] g[11.127,10.213]\n",
      "Step 382 took 32.06603002548218 seconds, dA[0.159,0.033] dB[0.056,0.123] g[8.598,8.676]\n",
      "Step 383 took 31.789466381072998 seconds, dA[0.105,0.035] dB[0.036,0.125] g[13.199,11.175]\n",
      "Step 384 took 31.577592849731445 seconds, dA[0.037,0.158] dB[0.052,0.068] g[14.486,13.580]\n",
      "Step 385 took 32.41608452796936 seconds, dA[0.202,0.058] dB[0.026,0.056] g[10.763,10.252]\n",
      "Step 386 took 31.349720239639282 seconds, dA[0.053,0.063] dB[0.021,0.038] g[10.361,10.065]\n",
      "Step 387 took 30.51316523551941 seconds, dA[0.024,0.021] dB[0.024,0.056] g[14.161,13.286]\n",
      "Step 388 took 30.546074867248535 seconds, dA[0.105,0.018] dB[0.019,0.021] g[12.338,10.450]\n",
      "Step 389 took 30.51399874687195 seconds, dA[0.106,0.034] dB[0.012,0.039] g[12.110,10.186]\n",
      "Step 390 took 31.25776505470276 seconds, dA[0.041,0.425] dB[0.030,0.049] g[14.057,13.142]\n",
      "Step 391 took 31.87193202972412 seconds, dA[0.147,0.263] dB[0.021,0.074] g[13.324,13.098]\n",
      "Step 392 took 31.84675359725952 seconds, dA[0.160,0.092] dB[0.029,0.024] g[10.359,9.952]\n",
      "Step 393 took 33.20486664772034 seconds, dA[0.021,0.070] dB[0.036,0.082] g[10.241,9.417]\n",
      "Step 394 took 31.180356979370117 seconds, dA[0.053,0.086] dB[0.063,0.038] g[12.034,10.306]\n",
      "Step 395 took 31.220789432525635 seconds, dA[0.015,0.032] dB[0.031,0.026] g[13.074,12.364]\n",
      "Step 396 took 31.793828010559082 seconds, dA[0.122,0.041] dB[0.020,0.018] g[11.171,10.197]\n",
      "Step 397 took 32.53399968147278 seconds, dA[0.034,0.056] dB[0.019,0.039] g[10.043,9.643]\n",
      "Step 398 took 32.173314809799194 seconds, dA[0.034,0.053] dB[0.035,0.023] g[12.746,12.749]\n",
      "Step 399 took 31.223313093185425 seconds, dA[0.112,0.022] dB[0.010,0.033] g[11.774,10.498]\n",
      "Step 400 took 31.606831312179565 seconds, dA[0.023,0.017] dB[0.032,0.096] g[12.132,10.919]\n",
      ">Saved: g_model_AtoB_000400.h5 and g_model_BtoA_000400.h5\n",
      "Step 401 took 31.108261823654175 seconds, dA[0.042,0.180] dB[0.031,0.087] g[10.780,9.152]\n",
      "Step 402 took 30.992067337036133 seconds, dA[0.053,0.122] dB[0.018,0.098] g[11.255,9.386]\n",
      "Step 403 took 31.236531496047974 seconds, dA[0.035,0.115] dB[0.017,0.097] g[11.258,12.281]\n",
      "Step 404 took 31.234046697616577 seconds, dA[0.189,0.113] dB[0.029,0.067] g[12.407,10.547]\n",
      "Step 405 took 31.08336639404297 seconds, dA[0.095,0.051] dB[0.017,0.061] g[11.956,10.095]\n",
      "Step 406 took 32.213369846343994 seconds, dA[0.079,0.371] dB[0.024,0.038] g[11.996,9.395]\n",
      "Step 407 took 32.56180930137634 seconds, dA[0.082,0.038] dB[0.031,0.046] g[10.700,10.263]\n",
      "Step 408 took 30.96732521057129 seconds, dA[0.110,0.021] dB[0.013,0.082] g[11.688,10.342]\n",
      "Step 409 took 31.188438177108765 seconds, dA[0.061,0.083] dB[0.048,0.035] g[12.351,12.623]\n",
      "Step 410 took 31.306436777114868 seconds, dA[0.031,0.127] dB[0.033,0.040] g[11.806,10.899]\n",
      "Step 411 took 31.069618940353394 seconds, dA[0.042,0.044] dB[0.017,0.033] g[12.811,11.720]\n",
      "Step 412 took 30.863503217697144 seconds, dA[0.051,0.191] dB[0.011,0.027] g[13.184,12.575]\n",
      "Step 413 took 31.02966809272766 seconds, dA[0.041,0.179] dB[0.007,0.041] g[12.278,10.939]\n",
      "Step 414 took 31.607418060302734 seconds, dA[0.100,0.026] dB[0.023,0.038] g[12.769,11.025]\n",
      "Step 415 took 31.596941471099854 seconds, dA[0.094,0.194] dB[0.033,0.054] g[10.430,9.155]\n",
      "Step 416 took 31.309316873550415 seconds, dA[0.024,0.063] dB[0.017,0.035] g[13.329,12.465]\n",
      "Step 417 took 31.305424213409424 seconds, dA[0.066,0.070] dB[0.011,0.047] g[11.863,9.676]\n",
      "Step 418 took 31.493105173110962 seconds, dA[0.081,0.090] dB[0.029,0.034] g[12.434,11.055]\n",
      "Step 419 took 31.548646688461304 seconds, dA[0.207,0.185] dB[0.018,0.043] g[12.103,10.989]\n",
      "Step 420 took 31.393709182739258 seconds, dA[0.091,0.094] dB[0.015,0.045] g[10.191,9.229]\n",
      "Step 421 took 31.2470383644104 seconds, dA[0.117,0.101] dB[0.035,0.033] g[12.269,10.387]\n",
      "Step 422 took 31.15347909927368 seconds, dA[0.044,0.181] dB[0.017,0.037] g[12.372,10.738]\n",
      "Step 423 took 31.06176519393921 seconds, dA[0.088,0.067] dB[0.018,0.028] g[11.548,10.774]\n",
      "Step 424 took 31.680997371673584 seconds, dA[0.067,0.142] dB[0.014,0.075] g[13.547,13.445]\n",
      "Step 425 took 31.198644161224365 seconds, dA[0.065,0.050] dB[0.075,0.043] g[11.920,10.005]\n",
      "Step 426 took 31.60508680343628 seconds, dA[0.078,0.103] dB[0.017,0.030] g[11.507,9.347]\n",
      "Step 427 took 31.38949728012085 seconds, dA[0.079,0.263] dB[0.017,0.019] g[12.004,10.229]\n",
      "Step 428 took 31.6611430644989 seconds, dA[0.092,0.060] dB[0.038,0.028] g[13.381,14.291]\n",
      "Step 429 took 31.371822118759155 seconds, dA[0.072,0.167] dB[0.058,0.091] g[12.353,12.500]\n",
      "Step 430 took 30.96755290031433 seconds, dA[0.098,0.055] dB[0.082,0.045] g[13.756,12.013]\n",
      "Step 431 took 30.980586290359497 seconds, dA[0.085,0.052] dB[0.116,0.192] g[10.719,9.661]\n",
      "Step 432 took 30.677866458892822 seconds, dA[0.050,0.071] dB[0.031,0.090] g[11.919,10.338]\n",
      "Step 433 took 30.25804376602173 seconds, dA[0.071,0.109] dB[0.016,0.027] g[13.457,12.729]\n",
      "Step 434 took 30.566672563552856 seconds, dA[0.033,0.035] dB[0.012,0.033] g[12.690,11.664]\n",
      "Step 435 took 30.790151834487915 seconds, dA[0.055,0.038] dB[0.022,0.030] g[12.331,10.553]\n",
      "Step 436 took 31.05696201324463 seconds, dA[0.021,0.066] dB[0.023,0.031] g[12.185,12.328]\n",
      "Step 437 took 30.981412410736084 seconds, dA[0.075,0.089] dB[0.028,0.027] g[12.536,10.878]\n",
      "Step 438 took 32.28764533996582 seconds, dA[0.246,0.168] dB[0.031,0.058] g[11.219,10.049]\n",
      "Step 439 took 32.180853843688965 seconds, dA[0.019,0.011] dB[0.041,0.031] g[11.534,9.447]\n",
      "Step 440 took 31.6989004611969 seconds, dA[0.016,0.013] dB[0.038,0.047] g[11.175,10.000]\n",
      "Step 441 took 31.844840049743652 seconds, dA[0.014,0.095] dB[0.027,0.021] g[11.284,10.358]\n",
      "Step 442 took 32.88757824897766 seconds, dA[0.023,0.048] dB[0.010,0.021] g[13.205,12.026]\n",
      "Step 443 took 31.215564727783203 seconds, dA[0.044,0.080] dB[0.017,0.046] g[12.695,11.944]\n",
      "Step 444 took 30.689231395721436 seconds, dA[0.056,0.062] dB[0.022,0.034] g[11.826,10.300]\n",
      "Step 445 took 34.17256045341492 seconds, dA[0.035,0.039] dB[0.016,0.109] g[8.460,7.940]\n",
      "Step 446 took 34.705806016922 seconds, dA[0.034,0.114] dB[0.015,0.084] g[12.062,10.579]\n",
      "Step 447 took 34.46039295196533 seconds, dA[0.077,0.034] dB[0.035,0.052] g[11.644,10.597]\n",
      "Step 448 took 34.02714729309082 seconds, dA[0.018,0.091] dB[0.036,0.073] g[10.566,9.493]\n",
      "Step 449 took 31.95235323905945 seconds, dA[0.075,0.050] dB[0.018,0.142] g[10.137,9.286]\n",
      "Step 450 took 31.236999034881592 seconds, dA[0.018,0.093] dB[0.016,0.084] g[12.017,10.859]\n",
      "Step 451 took 31.572000741958618 seconds, dA[0.045,0.034] dB[0.197,0.079] g[11.962,10.069]\n",
      "Step 452 took 32.81696605682373 seconds, dA[0.060,0.221] dB[0.112,0.074] g[11.885,10.034]\n",
      "Step 453 took 34.05559062957764 seconds, dA[0.123,0.041] dB[0.137,0.152] g[9.636,9.810]\n",
      "Step 454 took 34.05799388885498 seconds, dA[0.056,0.028] dB[0.042,0.062] g[11.138,9.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 455 took 32.05715274810791 seconds, dA[0.034,0.032] dB[0.041,0.039] g[12.089,10.933]\n",
      "Step 456 took 31.22704815864563 seconds, dA[0.055,0.012] dB[0.013,0.071] g[12.625,11.482]\n",
      "Step 457 took 34.145774126052856 seconds, dA[0.027,0.042] dB[0.031,0.086] g[11.164,10.651]\n",
      "Step 458 took 34.434972524642944 seconds, dA[0.030,0.075] dB[0.033,0.055] g[12.103,10.527]\n",
      "Step 459 took 32.22566342353821 seconds, dA[0.022,0.030] dB[0.011,0.074] g[11.578,10.378]\n",
      "Step 460 took 31.25399947166443 seconds, dA[0.057,0.025] dB[0.081,0.043] g[11.074,10.278]\n",
      "Step 461 took 31.31400156021118 seconds, dA[0.055,0.044] dB[0.034,0.058] g[8.808,8.617]\n",
      "Step 462 took 31.258998155593872 seconds, dA[0.025,0.045] dB[0.017,0.042] g[11.908,10.651]\n",
      "Step 463 took 31.145655155181885 seconds, dA[0.033,0.016] dB[0.027,0.049] g[9.247,10.016]\n",
      "Step 464 took 31.191694259643555 seconds, dA[0.151,0.036] dB[0.025,0.038] g[12.114,10.360]\n",
      "Step 465 took 32.55255389213562 seconds, dA[0.077,0.030] dB[0.026,0.029] g[11.749,11.531]\n",
      "Step 466 took 34.28707027435303 seconds, dA[0.027,0.022] dB[0.021,0.060] g[13.182,12.929]\n",
      "Step 467 took 32.30768370628357 seconds, dA[0.050,0.083] dB[0.147,0.050] g[10.228,9.006]\n",
      "Step 468 took 30.82699942588806 seconds, dA[0.163,0.017] dB[0.023,0.026] g[10.684,9.538]\n",
      "Step 469 took 30.72200059890747 seconds, dA[0.025,0.308] dB[0.014,0.027] g[13.696,12.658]\n",
      "Step 470 took 31.76027798652649 seconds, dA[0.235,0.032] dB[0.011,0.036] g[12.332,11.478]\n",
      "Step 471 took 30.6117901802063 seconds, dA[0.071,0.115] dB[0.011,0.021] g[11.620,10.774]\n",
      "Step 472 took 30.543097972869873 seconds, dA[0.087,0.030] dB[0.010,0.035] g[10.287,9.763]\n",
      "Step 473 took 30.456998586654663 seconds, dA[0.014,0.030] dB[0.007,0.030] g[11.485,10.583]\n",
      "Step 474 took 30.403772830963135 seconds, dA[0.064,0.063] dB[0.013,0.055] g[11.266,10.186]\n",
      "Step 475 took 30.5524320602417 seconds, dA[0.041,0.094] dB[0.027,0.037] g[12.016,10.096]\n",
      "Step 476 took 30.769031286239624 seconds, dA[0.131,0.029] dB[0.047,0.076] g[7.741,7.233]\n",
      "Step 477 took 30.865000247955322 seconds, dA[0.015,0.023] dB[0.050,0.079] g[9.630,8.543]\n",
      "Step 478 took 31.104830980300903 seconds, dA[0.019,0.111] dB[0.038,0.039] g[10.687,8.910]\n",
      "Step 479 took 30.74399971961975 seconds, dA[0.068,0.058] dB[0.017,0.046] g[11.514,9.642]\n",
      "Step 480 took 30.418002605438232 seconds, dA[0.048,0.084] dB[0.010,0.047] g[12.985,11.968]\n",
      "Step 481 took 30.510000228881836 seconds, dA[0.033,0.015] dB[0.018,0.019] g[12.819,13.276]\n",
      "Step 482 took 30.41266107559204 seconds, dA[0.039,0.037] dB[0.011,0.025] g[14.270,14.975]\n",
      "Step 483 took 30.532998085021973 seconds, dA[0.103,0.063] dB[0.009,0.066] g[9.718,9.777]\n",
      "Step 484 took 30.612998962402344 seconds, dA[0.047,0.096] dB[0.034,0.039] g[10.895,9.335]\n",
      "Step 485 took 30.55699872970581 seconds, dA[0.031,0.030] dB[0.026,0.038] g[10.471,9.806]\n",
      "Step 486 took 30.588459491729736 seconds, dA[0.064,0.023] dB[0.009,0.013] g[9.891,9.571]\n",
      "Step 487 took 30.37799882888794 seconds, dA[0.045,0.111] dB[0.011,0.109] g[11.353,10.269]\n",
      "Step 488 took 30.488683223724365 seconds, dA[0.123,0.041] dB[0.096,0.026] g[11.701,10.335]\n",
      "Step 489 took 30.34700107574463 seconds, dA[0.053,0.069] dB[0.019,0.023] g[13.110,11.800]\n",
      "Step 490 took 30.99956226348877 seconds, dA[0.179,0.016] dB[0.009,0.021] g[11.491,10.228]\n",
      "Step 491 took 30.93216824531555 seconds, dA[0.060,0.073] dB[0.005,0.025] g[12.325,10.957]\n",
      "Step 492 took 30.452998876571655 seconds, dA[0.034,0.087] dB[0.043,0.030] g[11.890,12.481]\n",
      "Step 493 took 31.044001817703247 seconds, dA[0.030,0.026] dB[0.037,0.062] g[11.823,11.085]\n",
      "Step 494 took 30.56195616722107 seconds, dA[0.018,0.017] dB[0.025,0.049] g[10.792,9.806]\n",
      "Step 495 took 30.58900213241577 seconds, dA[0.096,0.103] dB[0.020,0.020] g[10.964,9.212]\n",
      "Step 496 took 30.58099889755249 seconds, dA[0.020,0.047] dB[0.019,0.027] g[11.159,10.164]\n",
      "Step 497 took 30.37200140953064 seconds, dA[0.029,0.092] dB[0.008,0.055] g[11.464,10.341]\n",
      "Step 498 took 30.592220067977905 seconds, dA[0.071,0.048] dB[0.062,0.033] g[8.805,8.436]\n",
      "Step 499 took 30.678000926971436 seconds, dA[0.043,0.025] dB[0.016,0.073] g[9.256,8.813]\n",
      "Step 500 took 30.625000476837158 seconds, dA[0.041,0.019] dB[0.016,0.044] g[12.011,10.488]\n",
      ">Saved: g_model_AtoB_000500.h5 and g_model_BtoA_000500.h5\n",
      "Step 501 took 30.65167498588562 seconds, dA[0.035,0.034] dB[0.014,0.049] g[11.303,9.941]\n",
      "Step 502 took 30.63896870613098 seconds, dA[0.043,0.064] dB[0.036,0.051] g[10.280,9.483]\n",
      "Step 503 took 30.506813287734985 seconds, dA[0.017,0.023] dB[0.011,0.045] g[11.006,9.140]\n",
      "Step 504 took 30.52100157737732 seconds, dA[0.013,0.046] dB[0.008,0.034] g[11.020,9.127]\n",
      "Step 505 took 30.497302055358887 seconds, dA[0.027,0.053] dB[0.004,0.014] g[11.554,10.207]\n",
      "Step 506 took 30.35813283920288 seconds, dA[0.069,0.053] dB[0.004,0.043] g[12.074,11.332]\n",
      "Step 507 took 30.455000162124634 seconds, dA[0.043,0.041] dB[0.036,0.031] g[10.777,9.557]\n",
      "Step 508 took 30.96400284767151 seconds, dA[0.023,0.023] dB[0.010,0.018] g[11.013,9.173]\n",
      "Step 509 took 30.87388825416565 seconds, dA[0.021,0.026] dB[0.007,0.051] g[12.269,10.936]\n",
      "Step 510 took 30.533799171447754 seconds, dA[0.016,0.068] dB[0.013,0.029] g[12.405,10.676]\n",
      "Step 511 took 30.44700002670288 seconds, dA[0.031,0.032] dB[0.018,0.018] g[11.547,9.821]\n",
      "Step 512 took 30.521514892578125 seconds, dA[0.048,0.082] dB[0.012,0.010] g[13.070,12.365]\n",
      "Step 513 took 30.517853260040283 seconds, dA[0.027,0.029] dB[0.005,0.018] g[13.593,13.408]\n",
      "Step 514 took 30.62580394744873 seconds, dA[0.158,0.147] dB[0.010,0.037] g[11.983,10.578]\n",
      "Step 515 took 30.451244831085205 seconds, dA[0.066,0.055] dB[0.022,0.019] g[10.184,9.576]\n",
      "Step 516 took 30.361998319625854 seconds, dA[0.014,0.033] dB[0.013,0.071] g[9.383,9.870]\n",
      "Step 517 took 30.465868711471558 seconds, dA[0.022,0.063] dB[0.020,0.040] g[9.931,9.142]\n",
      "Step 518 took 30.63300061225891 seconds, dA[0.336,0.062] dB[0.010,0.036] g[9.199,8.753]\n",
      "Step 519 took 30.57999849319458 seconds, dA[0.057,0.027] dB[0.009,0.043] g[11.910,10.575]\n",
      "Step 520 took 30.4868323802948 seconds, dA[0.030,0.092] dB[0.023,0.027] g[11.302,10.219]\n",
      "Step 521 took 30.665035247802734 seconds, dA[0.032,0.028] dB[0.015,0.012] g[10.154,9.756]\n",
      "Step 522 took 30.473000049591064 seconds, dA[0.080,0.028] dB[0.007,0.015] g[8.087,7.836]\n",
      "Step 523 took 30.639000177383423 seconds, dA[0.022,0.057] dB[0.007,0.012] g[13.085,12.030]\n",
      "Step 524 took 30.538999557495117 seconds, dA[0.020,0.186] dB[0.008,0.013] g[13.598,12.529]\n",
      "Step 525 took 30.592303037643433 seconds, dA[0.169,0.040] dB[0.008,0.019] g[11.794,10.490]\n",
      "Step 526 took 30.409000158309937 seconds, dA[0.042,0.072] dB[0.010,0.030] g[11.485,10.098]\n",
      "Step 527 took 30.428001165390015 seconds, dA[0.045,0.030] dB[0.013,0.015] g[9.703,9.379]\n",
      "Step 528 took 30.610999822616577 seconds, dA[0.025,0.056] dB[0.014,0.013] g[11.433,10.346]\n",
      "Step 529 took 30.594791173934937 seconds, dA[0.060,0.033] dB[0.009,0.023] g[10.306,9.446]\n",
      "Step 530 took 30.533999919891357 seconds, dA[0.040,0.240] dB[0.005,0.023] g[11.645,9.770]\n",
      "Step 531 took 30.61300230026245 seconds, dA[0.263,0.025] dB[0.006,0.016] g[8.997,9.054]\n",
      "Step 532 took 30.53000044822693 seconds, dA[0.053,0.020] dB[0.007,0.031] g[11.829,10.889]\n",
      "Step 533 took 30.546436548233032 seconds, dA[0.134,0.047] dB[0.015,0.013] g[11.223,9.742]\n",
      "Step 534 took 30.568998336791992 seconds, dA[0.028,0.059] dB[0.007,0.016] g[11.787,10.169]\n",
      "Step 535 took 30.599003314971924 seconds, dA[0.019,0.026] dB[0.021,0.022] g[11.566,10.322]\n",
      "Step 536 took 30.69256854057312 seconds, dA[0.027,0.020] dB[0.013,0.026] g[11.518,10.574]\n",
      "Step 537 took 30.82373881340027 seconds, dA[0.051,0.060] dB[0.011,0.035] g[12.776,12.535]\n",
      "Step 538 took 30.57400155067444 seconds, dA[0.106,0.113] dB[0.016,0.064] g[11.516,10.540]\n",
      "Step 539 took 30.55299949645996 seconds, dA[0.043,0.230] dB[0.015,0.057] g[11.742,10.794]\n",
      "Step 540 took 30.38619041442871 seconds, dA[0.043,0.038] dB[0.012,0.064] g[11.459,11.048]\n",
      "Step 541 took 30.618038177490234 seconds, dA[0.101,0.042] dB[0.032,0.112] g[10.289,9.718]\n",
      "Step 542 took 30.49070906639099 seconds, dA[0.029,0.025] dB[0.090,0.039] g[9.494,8.212]\n",
      "Step 543 took 30.490143299102783 seconds, dA[0.012,0.108] dB[0.028,0.034] g[11.264,9.510]\n",
      "Step 544 took 30.508774995803833 seconds, dA[0.014,0.011] dB[0.020,0.019] g[9.272,8.540]\n",
      "Step 545 took 30.455374240875244 seconds, dA[0.146,0.063] dB[0.020,0.010] g[10.006,9.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546 took 30.52699899673462 seconds, dA[0.021,0.045] dB[0.015,0.021] g[10.954,9.297]\n",
      "Step 547 took 30.562999486923218 seconds, dA[0.096,0.011] dB[0.036,0.022] g[9.723,9.125]\n",
      "Step 548 took 30.557188987731934 seconds, dA[0.011,0.030] dB[0.017,0.019] g[11.327,9.974]\n",
      "Step 549 took 30.91476845741272 seconds, dA[0.049,0.175] dB[0.010,0.023] g[11.444,9.488]\n",
      "Step 550 took 32.292157888412476 seconds, dA[0.039,0.070] dB[0.007,0.016] g[12.335,11.595]\n",
      "Step 551 took 32.074384450912476 seconds, dA[0.219,0.060] dB[0.008,0.019] g[10.766,10.211]\n",
      "Step 552 took 31.818267345428467 seconds, dA[0.165,0.067] dB[0.011,0.024] g[11.960,10.480]\n",
      "Step 553 took 31.771782636642456 seconds, dA[0.026,0.015] dB[0.013,0.017] g[10.500,9.018]\n",
      "Step 554 took 31.95163607597351 seconds, dA[0.016,0.014] dB[0.006,0.026] g[11.225,9.677]\n",
      "Step 555 took 32.311567544937134 seconds, dA[0.017,0.006] dB[0.020,0.020] g[10.570,9.425]\n",
      "Step 556 took 31.85101056098938 seconds, dA[0.008,0.094] dB[0.005,0.009] g[13.274,12.475]\n",
      "Step 557 took 32.121676206588745 seconds, dA[0.023,0.035] dB[0.006,0.014] g[13.484,13.016]\n",
      "Step 558 took 32.8371467590332 seconds, dA[0.030,0.023] dB[0.023,0.011] g[8.798,8.055]\n",
      "Step 559 took 32.18361687660217 seconds, dA[0.036,0.074] dB[0.027,0.032] g[12.878,12.320]\n",
      "Step 560 took 31.82591676712036 seconds, dA[0.113,0.029] dB[0.018,0.018] g[11.572,10.235]\n",
      "Step 561 took 31.77351713180542 seconds, dA[0.062,0.011] dB[0.009,0.011] g[11.299,10.363]\n",
      "Step 562 took 31.71126890182495 seconds, dA[0.017,0.075] dB[0.005,0.010] g[10.960,8.853]\n",
      "Step 563 took 31.758763551712036 seconds, dA[0.280,0.162] dB[0.005,0.012] g[11.261,9.545]\n",
      "Step 564 took 31.753238677978516 seconds, dA[0.026,0.025] dB[0.004,0.014] g[11.896,10.535]\n",
      "Step 565 took 31.616997957229614 seconds, dA[0.016,0.011] dB[0.002,0.032] g[11.825,10.209]\n",
      "Step 566 took 32.21800208091736 seconds, dA[0.024,0.067] dB[0.018,0.028] g[8.771,8.247]\n",
      "Step 567 took 31.757879734039307 seconds, dA[0.025,0.048] dB[0.017,0.032] g[11.924,10.102]\n",
      "Step 568 took 32.32720756530762 seconds, dA[0.018,0.027] dB[0.012,0.018] g[11.181,9.867]\n",
      "Step 569 took 32.137999057769775 seconds, dA[0.013,0.031] dB[0.013,0.021] g[12.225,12.393]\n",
      "Step 570 took 31.92200231552124 seconds, dA[0.119,0.028] dB[0.015,0.030] g[11.146,9.789]\n",
      "Step 571 took 32.10860252380371 seconds, dA[0.026,0.281] dB[0.012,0.037] g[11.301,9.263]\n",
      "Step 572 took 31.74502944946289 seconds, dA[0.201,0.139] dB[0.013,0.016] g[10.508,8.554]\n",
      "Step 573 took 33.547521352767944 seconds, dA[0.029,0.069] dB[0.011,0.028] g[11.622,12.082]\n",
      "Step 574 took 32.331932067871094 seconds, dA[0.065,0.239] dB[0.006,0.017] g[8.119,7.360]\n",
      "Step 575 took 30.696802139282227 seconds, dA[0.315,0.043] dB[0.011,0.017] g[11.240,10.217]\n",
      "Step 576 took 30.519001007080078 seconds, dA[0.025,0.129] dB[0.005,0.023] g[11.581,9.837]\n",
      "Step 577 took 30.383999824523926 seconds, dA[0.037,0.020] dB[0.005,0.024] g[11.743,10.721]\n",
      "Step 578 took 30.67271590232849 seconds, dA[0.129,0.014] dB[0.027,0.010] g[10.415,9.517]\n",
      "Step 579 took 30.570684671401978 seconds, dA[0.070,0.060] dB[0.009,0.013] g[8.990,8.355]\n",
      "Step 580 took 30.566999673843384 seconds, dA[0.089,0.011] dB[0.004,0.021] g[9.318,8.861]\n",
      "Step 581 took 30.489999532699585 seconds, dA[0.037,0.157] dB[0.006,0.019] g[11.679,9.816]\n",
      "Step 582 took 30.53160548210144 seconds, dA[0.043,0.047] dB[0.005,0.016] g[11.391,9.915]\n",
      "Step 583 took 30.390040397644043 seconds, dA[0.187,0.396] dB[0.005,0.027] g[10.753,9.195]\n",
      "Step 584 took 30.680999994277954 seconds, dA[0.141,0.022] dB[0.008,0.008] g[10.937,8.971]\n",
      "Step 585 took 30.348999977111816 seconds, dA[0.081,0.035] dB[0.014,0.007] g[9.847,8.912]\n",
      "Step 586 took 31.327046394348145 seconds, dA[0.289,0.132] dB[0.010,0.017] g[9.055,7.675]\n",
      "Step 587 took 30.778369665145874 seconds, dA[0.093,0.083] dB[0.006,0.011] g[11.129,9.159]\n",
      "Step 588 took 30.65904688835144 seconds, dA[0.050,0.045] dB[0.004,0.026] g[11.301,10.243]\n",
      "Step 589 took 30.648001670837402 seconds, dA[0.046,0.050] dB[0.017,0.011] g[9.320,8.649]\n",
      "Step 590 took 30.639172077178955 seconds, dA[0.052,0.158] dB[0.009,0.009] g[10.846,9.414]\n",
      "Step 591 took 30.44921040534973 seconds, dA[0.040,0.025] dB[0.004,0.016] g[11.545,9.736]\n",
      "Step 592 took 30.737618923187256 seconds, dA[0.020,0.017] dB[0.011,0.010] g[10.724,9.512]\n",
      "Step 593 took 30.487001419067383 seconds, dA[0.017,0.021] dB[0.007,0.020] g[11.748,9.984]\n",
      "Step 594 took 30.543888807296753 seconds, dA[0.038,0.019] dB[0.006,0.032] g[11.542,9.287]\n",
      "Step 595 took 30.744001150131226 seconds, dA[0.021,0.278] dB[0.010,0.025] g[8.180,7.231]\n",
      "Step 596 took 30.72599959373474 seconds, dA[0.329,0.083] dB[0.016,0.031] g[9.859,9.764]\n",
      "Step 597 took 30.630999326705933 seconds, dA[0.085,0.077] dB[0.014,0.061] g[11.124,9.740]\n",
      "Step 598 took 30.50316047668457 seconds, dA[0.071,0.067] dB[0.044,0.031] g[10.281,9.353]\n",
      "Step 599 took 30.463998079299927 seconds, dA[0.068,0.147] dB[0.023,0.037] g[10.835,9.335]\n",
      "Step 600 took 30.65800142288208 seconds, dA[0.027,0.023] dB[0.021,0.013] g[11.802,10.777]\n",
      ">Saved: g_model_AtoB_000600.h5 and g_model_BtoA_000600.h5\n",
      "Step 601 took 30.45060443878174 seconds, dA[0.106,0.022] dB[0.009,0.033] g[11.720,10.577]\n",
      "Step 602 took 30.90591549873352 seconds, dA[0.041,0.068] dB[0.027,0.020] g[10.938,9.685]\n",
      "Step 603 took 30.73600149154663 seconds, dA[0.014,0.025] dB[0.012,0.016] g[12.343,11.658]\n",
      "Step 604 took 30.618999481201172 seconds, dA[0.056,0.098] dB[0.006,0.016] g[10.236,9.244]\n",
      "Step 605 took 30.68962836265564 seconds, dA[0.063,0.014] dB[0.011,0.023] g[10.741,11.450]\n",
      "Step 606 took 30.797916650772095 seconds, dA[0.242,0.031] dB[0.012,0.029] g[8.474,7.317]\n",
      "Step 607 took 30.575998544692993 seconds, dA[0.254,0.211] dB[0.010,0.059] g[12.316,11.301]\n",
      "Step 608 took 30.625002145767212 seconds, dA[0.088,0.025] dB[0.018,0.033] g[8.880,8.717]\n",
      "Step 609 took 30.69851803779602 seconds, dA[0.332,0.095] dB[0.011,0.015] g[11.078,10.225]\n",
      "Step 610 took 30.49560236930847 seconds, dA[0.046,0.086] dB[0.006,0.042] g[11.527,10.459]\n",
      "Step 611 took 30.59409785270691 seconds, dA[0.102,0.055] dB[0.021,0.026] g[11.033,9.614]\n",
      "Step 612 took 30.479998111724854 seconds, dA[0.037,0.028] dB[0.016,0.008] g[10.254,10.439]\n",
      "Step 613 took 30.62375044822693 seconds, dA[0.057,0.040] dB[0.011,0.022] g[10.297,9.364]\n",
      "Step 614 took 30.644538640975952 seconds, dA[0.014,0.029] dB[0.005,0.030] g[10.979,9.021]\n",
      "Step 615 took 30.63200283050537 seconds, dA[0.025,0.019] dB[0.010,0.008] g[8.130,7.198]\n",
      "Step 616 took 30.443997621536255 seconds, dA[0.036,0.094] dB[0.009,0.019] g[11.058,9.503]\n",
      "Step 617 took 30.517164707183838 seconds, dA[0.203,0.017] dB[0.006,0.013] g[10.796,10.012]\n",
      "Step 618 took 30.543027639389038 seconds, dA[0.063,0.348] dB[0.004,0.021] g[10.412,8.465]\n",
      "Step 619 took 30.492998361587524 seconds, dA[0.032,0.039] dB[0.010,0.012] g[9.527,8.495]\n",
      "Step 620 took 30.859538078308105 seconds, dA[0.080,0.041] dB[0.008,0.025] g[11.256,10.281]\n",
      "Step 621 took 30.774850368499756 seconds, dA[0.038,0.070] dB[0.017,0.013] g[11.448,10.360]\n",
      "Step 622 took 30.66493034362793 seconds, dA[0.023,0.076] dB[0.007,0.011] g[12.395,12.635]\n",
      "Step 623 took 30.62399983406067 seconds, dA[0.056,0.122] dB[0.007,0.028] g[11.327,10.036]\n",
      "Step 624 took 30.51799964904785 seconds, dA[0.057,0.044] dB[0.011,0.028] g[10.761,8.982]\n",
      "Step 625 took 30.702553749084473 seconds, dA[0.087,0.077] dB[0.007,0.031] g[11.189,9.574]\n",
      "Step 626 took 30.629526615142822 seconds, dA[0.063,0.099] dB[0.005,0.021] g[9.177,8.101]\n",
      "Step 627 took 30.592000007629395 seconds, dA[0.063,0.040] dB[0.013,0.012] g[8.556,7.600]\n",
      "Step 628 took 30.567001819610596 seconds, dA[0.045,0.050] dB[0.008,0.008] g[10.779,9.072]\n",
      "Step 629 took 30.956687450408936 seconds, dA[0.076,0.035] dB[0.005,0.007] g[12.131,10.954]\n",
      "Step 630 took 30.720035076141357 seconds, dA[0.054,0.026] dB[0.007,0.018] g[10.215,9.503]\n",
      "Step 631 took 30.459120512008667 seconds, dA[0.101,0.009] dB[0.011,0.039] g[9.332,8.596]\n",
      "Step 632 took 30.727998733520508 seconds, dA[0.146,0.479] dB[0.017,0.011] g[10.713,8.531]\n",
      "Step 633 took 30.69686508178711 seconds, dA[0.188,0.059] dB[0.005,0.013] g[11.093,9.167]\n",
      "Step 634 took 30.61699914932251 seconds, dA[0.035,0.016] dB[0.008,0.012] g[9.493,9.868]\n",
      "Step 635 took 30.708000898361206 seconds, dA[0.024,0.040] dB[0.009,0.015] g[11.393,9.518]\n",
      "Step 636 took 30.511998176574707 seconds, dA[0.018,0.023] dB[0.009,0.010] g[10.771,9.800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 637 took 30.52325963973999 seconds, dA[0.122,0.019] dB[0.003,0.006] g[10.014,8.604]\n",
      "Step 638 took 30.679000854492188 seconds, dA[0.027,0.022] dB[0.004,0.026] g[10.771,9.030]\n",
      "Step 639 took 30.480997800827026 seconds, dA[0.021,0.138] dB[0.003,0.012] g[10.777,8.654]\n",
      "Step 640 took 30.589999675750732 seconds, dA[0.185,0.029] dB[0.009,0.020] g[9.441,8.688]\n",
      "Step 641 took 30.760299921035767 seconds, dA[0.025,0.015] dB[0.007,0.011] g[11.068,9.640]\n",
      "Step 642 took 30.618001699447632 seconds, dA[0.032,0.116] dB[0.006,0.007] g[11.047,9.061]\n",
      "Step 643 took 30.457000255584717 seconds, dA[0.017,0.010] dB[0.012,0.007] g[12.610,12.463]\n",
      "Step 644 took 30.791000604629517 seconds, dA[0.023,0.015] dB[0.007,0.015] g[11.255,10.867]\n",
      "Step 645 took 30.690470933914185 seconds, dA[0.023,0.014] dB[0.013,0.016] g[9.797,9.352]\n",
      "Step 646 took 30.555053234100342 seconds, dA[0.017,0.048] dB[0.007,0.013] g[9.717,8.435]\n",
      "Step 647 took 30.449998378753662 seconds, dA[0.032,0.014] dB[0.009,0.008] g[9.006,8.703]\n",
      "Step 648 took 30.49199914932251 seconds, dA[0.049,0.009] dB[0.002,0.008] g[8.825,7.871]\n",
      "Step 649 took 30.962189197540283 seconds, dA[0.020,0.024] dB[0.004,0.007] g[10.868,8.849]\n",
      "Step 650 took 31.048046112060547 seconds, dA[0.017,0.050] dB[0.003,0.010] g[11.397,9.784]\n",
      "Step 651 took 30.638999223709106 seconds, dA[0.024,0.017] dB[0.006,0.011] g[9.635,8.895]\n",
      "Step 652 took 30.670193433761597 seconds, dA[0.021,0.027] dB[0.009,0.023] g[9.845,9.801]\n",
      "Step 653 took 30.592583656311035 seconds, dA[0.020,0.009] dB[0.008,0.016] g[8.312,7.869]\n",
      "Step 654 took 30.675002336502075 seconds, dA[0.017,0.021] dB[0.014,0.017] g[8.873,8.230]\n",
      "Step 655 took 30.578996896743774 seconds, dA[0.036,0.030] dB[0.009,0.014] g[9.435,8.770]\n",
      "Step 656 took 30.607186555862427 seconds, dA[0.022,0.008] dB[0.004,0.015] g[11.992,10.772]\n",
      "Step 657 took 30.720695972442627 seconds, dA[0.023,0.040] dB[0.007,0.025] g[12.683,11.840]\n",
      "Step 658 took 31.090569257736206 seconds, dA[0.018,0.062] dB[0.016,0.010] g[11.734,12.238]\n",
      "Step 659 took 30.537001848220825 seconds, dA[0.043,0.040] dB[0.004,0.008] g[8.143,7.661]\n",
      "Step 660 took 30.678157091140747 seconds, dA[0.017,0.011] dB[0.011,0.009] g[9.798,8.604]\n",
      "Step 661 took 30.741554498672485 seconds, dA[0.102,0.089] dB[0.012,0.018] g[9.921,9.698]\n",
      "Step 662 took 30.654000759124756 seconds, dA[0.284,0.037] dB[0.008,0.012] g[11.002,10.107]\n",
      "Step 663 took 30.6589994430542 seconds, dA[0.035,0.045] dB[0.008,0.009] g[10.837,9.693]\n",
      "Step 664 took 30.72784972190857 seconds, dA[0.026,0.105] dB[0.009,0.012] g[10.914,9.479]\n",
      "Step 665 took 30.584993362426758 seconds, dA[0.057,0.009] dB[0.007,0.008] g[9.492,9.327]\n",
      "Step 666 took 30.542068481445312 seconds, dA[0.050,0.093] dB[0.008,0.010] g[11.386,10.387]\n",
      "Step 667 took 30.61704707145691 seconds, dA[0.100,0.069] dB[0.003,0.011] g[11.059,9.610]\n",
      "Step 668 took 32.048229455947876 seconds, dA[0.021,0.017] dB[0.002,0.018] g[10.754,9.409]\n",
      "Step 669 took 31.49722933769226 seconds, dA[0.175,0.025] dB[0.013,0.012] g[10.068,8.919]\n",
      "Step 670 took 31.29948925971985 seconds, dA[0.167,0.025] dB[0.007,0.014] g[10.251,8.450]\n",
      "Step 671 took 31.693421840667725 seconds, dA[0.009,0.038] dB[0.009,0.008] g[12.736,11.918]\n",
      "Step 672 took 31.809627532958984 seconds, dA[0.277,0.030] dB[0.006,0.011] g[10.105,9.162]\n",
      "Step 673 took 31.156151056289673 seconds, dA[0.017,0.038] dB[0.011,0.007] g[9.006,7.557]\n",
      "Step 674 took 31.5861918926239 seconds, dA[0.032,0.067] dB[0.005,0.008] g[11.042,9.171]\n",
      "Step 675 took 32.81445002555847 seconds, dA[0.035,0.102] dB[0.007,0.011] g[11.276,9.686]\n",
      "Step 676 took 32.23073744773865 seconds, dA[0.009,0.205] dB[0.008,0.013] g[10.862,9.748]\n",
      "Step 677 took 31.718844175338745 seconds, dA[0.045,0.009] dB[0.003,0.022] g[11.887,10.474]\n",
      "Step 678 took 31.800074577331543 seconds, dA[0.174,0.034] dB[0.007,0.024] g[9.690,8.634]\n",
      "Step 679 took 31.83903479576111 seconds, dA[0.048,0.253] dB[0.010,0.012] g[11.242,8.987]\n",
      "Step 680 took 32.44752550125122 seconds, dA[0.143,0.039] dB[0.003,0.012] g[10.865,9.974]\n",
      "Step 681 took 30.331159353256226 seconds, dA[0.076,0.220] dB[0.007,0.018] g[10.516,8.856]\n",
      "Step 682 took 30.5239999294281 seconds, dA[0.099,0.035] dB[0.009,0.017] g[11.432,10.396]\n",
      "Step 683 took 30.54232168197632 seconds, dA[0.039,0.009] dB[0.007,0.008] g[10.781,10.141]\n",
      "Step 684 took 30.692041397094727 seconds, dA[0.053,0.029] dB[0.014,0.011] g[10.779,9.482]\n",
      "Step 685 took 30.566517114639282 seconds, dA[0.085,0.025] dB[0.007,0.014] g[12.190,11.268]\n",
      "Step 686 took 30.57599925994873 seconds, dA[0.199,0.025] dB[0.006,0.013] g[10.790,9.497]\n",
      "Step 687 took 30.437262773513794 seconds, dA[0.027,0.026] dB[0.005,0.020] g[8.060,7.351]\n",
      "Step 688 took 30.930787801742554 seconds, dA[0.020,0.137] dB[0.003,0.005] g[11.366,9.586]\n",
      "Step 689 took 31.89499831199646 seconds, dA[0.120,0.041] dB[0.002,0.013] g[9.904,8.808]\n",
      "Step 690 took 31.196001052856445 seconds, dA[0.031,0.023] dB[0.008,0.015] g[12.069,11.652]\n",
      "Step 691 took 30.91573977470398 seconds, dA[0.020,0.023] dB[0.010,0.008] g[12.656,12.168]\n",
      "Step 692 took 31.0509991645813 seconds, dA[0.021,0.006] dB[0.008,0.008] g[10.744,10.231]\n",
      "Step 693 took 33.611891984939575 seconds, dA[0.019,0.046] dB[0.005,0.011] g[11.245,10.306]\n",
      "Step 694 took 34.64450168609619 seconds, dA[0.045,0.019] dB[0.005,0.005] g[10.880,9.375]\n",
      "Step 695 took 33.234644651412964 seconds, dA[0.037,0.065] dB[0.002,0.005] g[11.088,9.565]\n",
      "Step 696 took 32.24818754196167 seconds, dA[0.014,0.016] dB[0.002,0.007] g[11.306,10.371]\n",
      "Step 697 took 30.658998489379883 seconds, dA[0.024,0.038] dB[0.002,0.009] g[10.677,9.440]\n",
      "Step 698 took 30.920050382614136 seconds, dA[0.245,0.106] dB[0.007,0.007] g[9.464,8.786]\n",
      "Step 699 took 30.628288745880127 seconds, dA[0.036,0.056] dB[0.007,0.010] g[11.062,9.377]\n",
      "Step 700 took 31.069000720977783 seconds, dA[0.227,0.064] dB[0.009,0.012] g[9.025,8.634]\n",
      ">Saved: g_model_AtoB_000700.h5 and g_model_BtoA_000700.h5\n",
      "Step 701 took 30.60318946838379 seconds, dA[0.024,0.014] dB[0.018,0.013] g[10.939,9.585]\n"
     ]
    }
   ],
   "source": [
    "# example of training a cyclegan on the horse2zebra dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from random import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    # define model\n",
    "    model = Model(in_image, patch_out)\n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "    return model\n",
    "\n",
    "# generator a resnet block\n",
    "def resnet_block(n_filters, input_layer):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # first layer convolutional layer\n",
    "    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # second convolutional layer\n",
    "    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    # concatenate merge channel-wise with input layer\n",
    "    g = Concatenate()([g, input_layer])\n",
    "    return g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape, n_resnet=9):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # c7s1-64\n",
    "    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d128\n",
    "    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d256\n",
    "    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # R256\n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(256, g)\n",
    "    # u128\n",
    "    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # u64\n",
    "    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # c7s1-3\n",
    "    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n",
    "\n",
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n",
    "    # ensure the model we're updating is trainable\n",
    "    g_model_1.trainable = True\n",
    "    # mark discriminator as not trainable\n",
    "    d_model.trainable = False\n",
    "    # mark other generator model as not trainable\n",
    "    g_model_2.trainable = False\n",
    "    # discriminator element\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "    # identity element\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "    # forward cycle\n",
    "    output_f = g_model_2(gen1_out)\n",
    "    # backward cycle\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "    # define model graph\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "    # define optimization algorithm configuration\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    # compile model with weighting of least squares loss and L1 loss\n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "    # load the dataset\n",
    "    data = load(filename)\n",
    "    # unpack arrays\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1, X2]\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "    # generate fake instance\n",
    "    X = g_model.predict(dataset)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y\n",
    "\n",
    "# save the generator models to file\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "    # save the first generator model\n",
    "    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "    g_model_AtoB.save(filename1)\n",
    "    # save the second generator model\n",
    "    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "    g_model_BtoA.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "    # select a sample of input images\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "    # generate translated images\n",
    "    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_in = (X_in + 1) / 2.0\n",
    "    X_out = (X_out + 1) / 2.0\n",
    "    # plot real images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_in[i])\n",
    "    # plot translated image\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_out[i])\n",
    "    # save plot to file\n",
    "    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "\n",
    "# update image pool for fake images\n",
    "def update_image_pool(pool, images, max_size=50):\n",
    "    selected = list()\n",
    "    for image in images:\n",
    "        if len(pool) < max_size:\n",
    "            # stock the pool\n",
    "            pool.append(image)\n",
    "            selected.append(image)\n",
    "        elif random() < 0.5:\n",
    "            # use image, but don't add it to the pool\n",
    "            selected.append(image)\n",
    "        else:\n",
    "            # replace an existing image and use replaced image\n",
    "            ix = randint(0, len(pool))\n",
    "            selected.append(pool[ix])\n",
    "            pool[ix] = image\n",
    "    return asarray(selected)\n",
    "\n",
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "    # define properties of the training run\n",
    "    n_epochs, n_batch, = 50, 1\n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model_A.output_shape[1]\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # prepare image pool for fakes\n",
    "    poolA, poolB = list(), list()\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    print(n_steps)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        import time\n",
    "        T1 = time.time()\n",
    "        # select a batch of real samples\n",
    "        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "        # update fakes from pool\n",
    "        X_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "        X_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "        # update generator B->A via adversarial and cycle loss\n",
    "        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "        # update discriminator for A -> [real/fake]\n",
    "        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "        # update generator A->B via adversarial and cycle loss\n",
    "        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "        # update discriminator for B -> [real/fake]\n",
    "        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "        # summarize performance\n",
    "        print('Step %d took %s seconds, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1,time.time()-T1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % 100 == 0:\n",
    "            # plot A->B translation\n",
    "            summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
    "            # plot B->A translation\n",
    "            summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n",
    "            # save the models\n",
    "            save_models(i, g_model_AtoB, g_model_BtoA)\n",
    "\n",
    "# load image data\n",
    "dataset = load_real_samples('maps_cycleGAN_256.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# generator: A -> B\n",
    "g_model_AtoB = define_generator(image_shape)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define_generator(image_shape)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define_discriminator(image_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define_discriminator(image_shape)\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
    "# train models\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
